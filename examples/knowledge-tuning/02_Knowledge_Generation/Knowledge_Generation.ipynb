{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced Summary Knowledge Tuning - Data Generation\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to generate high-quality knowledge tuning datasets using the SDG Hub framework. It creates multiple types of document augmentations and corresponding question-answer pairs that can be used to train or fine-tune language models for enhanced summarization and knowledge extraction capabilities.\n",
    "\n",
    "## What This Notebook Does\n",
    "\n",
    "This notebook will:\n",
    "\n",
    "2. **Generate Four Types of Knowledge Tuning Datasets**:\n",
    "   - **Extractive Summaries**: Concise summaries that extract key information directly from source documents\n",
    "   - **Detailed Summaries**: Comprehensive summaries that provide thorough coverage of document content\n",
    "   - **Key Facts**: Structured fact extraction with corresponding Q&A pairs\n",
    "   - **Document-Based Q&A**: Question-answer pairs generated directly from document content\n",
    "\n",
    "\n",
    "4. **Output Structured Training Data**:\n",
    "   - For each augmentation we save JSONL dataset.\n",
    "   - You can follow [knowledge_mixing](knowledge_mixing.ipynb) to convert it into training dataset\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- SDG Hub installed and configured\n",
    "- Environment variables set up (see [.env.example](.env.example)). Specifically set the model provider, seed data and output path.\n",
    "- Document pre-processing completed (run [document_pre_processing.ipynb](document_pre_processing.ipynb) first)\n",
    "\n",
    "```bash \n",
    "git clone https://github.com/Red-Hat-AI-Innovation-Team/sdg_hub.git\n",
    "cd sdg_hub\n",
    "pip install .[examples]\n",
    "copy the .env.example to .env and set the model endpoint and generation/mixing parameters\n",
    "```\n",
    "**⚠️ If you haven't already, run the document pre-processing notebook to create the seed data.**\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "After running this notebook, use [knowledge_mixing](knowledge_mixing.ipynb) to combine and curate the generated datasets for final model training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Paths and Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "WORKSPACE = Path.cwd().parent  # Path to the workspace directory\n",
    "\n",
    "OUTPUT_DIR = WORKSPACE / \"output\" / \"step_02\"\n",
    "\n",
    "OUTPUT_DIR.mkdir(\n",
    "    parents=True, exist_ok=True\n",
    ")  # Create the output directory if it doesn't exist\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teacher Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"openai/llama-4-scout-17b-16e-w4a16\"\n",
    "API_KEY = \"\"  # Provide your API key here\n",
    "ENDPOINT = \"https://llama-4-scout-17b-16e-w4a16-maas-apicast-production.apps.prod.rhoai.rh-aiservices-bu.com:443/v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SDG Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required to run the flow with async mode\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()  \n",
    "\n",
    "\n",
    "RUN_ON_VALIDATION_SET = True  # Whether to run the model on the validation set\n",
    "SEED_DATA_SUBSAMPLE = 0 # Subsample the seed data for faster testing. \n",
    "\n",
    "SEED_DATA_FILE = (\n",
    "    WORKSPACE / \"output\" / \"step_01\" / \"seed_data.jsonl\"\n",
    ")  # Path to the seed data file generated in step 1\n",
    "\n",
    "\n",
    "ENABLE_REASONING = False  # Whether to enable reasoning in the model\n",
    "NUMBER_OF_SUMMARIES = 10  # Number of summaries to generate\n",
    "MAX_CONCURRENCY = 50  # Maximum number of concurrent requests to the model\n",
    "\n",
    "\n",
    "\n",
    "if not SEED_DATA_FILE.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"\\nNot a valid seed data ! {SEED_DATA_FILE}.\\nPlease run step 1 to generate the seed data. \\n(or) Provide the correct path to the seed data file.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third Party\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# First Party\n",
    "from sdg_hub import Flow, FlowRegistry\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the seed data generated in step 1\n",
    "quality_corpus = load_dataset('json', data_files=f\"{SEED_DATA_FILE}\", split='train')\n",
    "\n",
    "if SEED_DATA_SUBSAMPLE > 0:\n",
    "    quality_corpus = quality_corpus.select(range(SEED_DATA_SUBSAMPLE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run SDG\n",
    "- This will create knowledge flow from provided yaml file\n",
    "- We will run this on small dataset for demo purposes\n",
    "- For large scale generation, please use the python command provided in the next cell\n",
    "- You can analyze the generated data to ensure the quality is similar to proivded QnA pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discover the available generation flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-discover all available flows (no setup needed!)\n",
    "FlowRegistry.discover_flows()\n",
    "\n",
    "# List available flows\n",
    "flows = FlowRegistry.list_flows()\n",
    "print(f\"Available flows: {flows}\")\n",
    "\n",
    "# You can also search the flows by tag\n",
    "qa_flows = FlowRegistry.search_flows(tag=\"question-generation\")\n",
    "print(f\"QA flows: {qa_flows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Extractive Summaries Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data for extractive summary\n",
    "flow_name = \"Extractive Summary Knowledge Tuning Dataset Generation Flow\"\n",
    "flow_path = FlowRegistry.get_flow_path(flow_name)\n",
    "flow = Flow.from_yaml(flow_path)\n",
    "\n",
    "# Set model configuration\n",
    "flow.set_model_config(\n",
    "    model=MODEL_NAME,\n",
    "    api_base=ENDPOINT,\n",
    "    api_key=API_KEY,\n",
    ")\n",
    "\n",
    "# Generate data for extractive summary\n",
    "if ENABLE_REASONING:\n",
    "    # Increase max tokens to accommodate reasoning content\n",
    "    runtime_params = {\n",
    "        'question_generation': {'max_tokens': 1024}, \n",
    "        'gen_extractive_summary': {'n': NUMBER_OF_SUMMARIES, 'max_tokens': 6000}\n",
    "        }\n",
    "else:\n",
    "    runtime_params = {\n",
    "        'gen_extractive_summary': {\n",
    "            'n': NUMBER_OF_SUMMARIES\n",
    "        }\n",
    "    }\n",
    "\n",
    "extractive_summary_generated_data = flow.generate(quality_corpus, runtime_params=runtime_params, max_concurrency=MAX_CONCURRENCY)\n",
    "\n",
    "extractive_summary_generated_data.to_json(os.path.join(OUTPUT_DIR, 'extractive_summary', 'gen.jsonl'), orient='records', lines=True)\n",
    "\n",
    "print(f\"✓ Extractive summary: {len(extractive_summary_generated_data)} records\")\n",
    "\n",
    "print(f\"✓ Columns: {list(extractive_summary_generated_data.column_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Detailed Summary Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate similar data for Detailed Summary\n",
    "flow_name = \"Detailed Summary Knowledge Tuning Dataset Generation Flow\"\n",
    "flow_path = FlowRegistry.get_flow_path(flow_name)\n",
    "flow = Flow.from_yaml(flow_path)\n",
    "\n",
    "# Set model configuration\n",
    "flow.set_model_config(\n",
    "    model=MODEL_NAME,\n",
    "    api_base=ENDPOINT,\n",
    "    api_key=API_KEY,\n",
    ")\n",
    "\n",
    "if ENABLE_REASONING:\n",
    "    # Increase max tokens to accommodate reasoning content\n",
    "    runtime_params = {\n",
    "        'question_generation': {'max_tokens': 1024}, \n",
    "        'gen_detailed_summary': {'n': NUMBER_OF_SUMMARIES, 'max_tokens': 6000}\n",
    "        }\n",
    "else:\n",
    "    runtime_params = ({'gen_detailed_summary': {\n",
    "        'n': NUMBER_OF_SUMMARIES\n",
    "    }})\n",
    "# Generate data for detailed summary\n",
    "detailed_summary_generated_data = flow.generate(quality_corpus, runtime_params=runtime_params, max_concurrency=MAX_CONCURRENCY)\n",
    "\n",
    "detailed_summary_generated_data.to_json(os.path.join(OUTPUT_DIR, 'detailed_summary', 'gen.jsonl'), orient='records', lines=True)\n",
    "\n",
    "print(f\"✓ Detailed summary: {len(detailed_summary_generated_data)} records\")\n",
    "\n",
    "print(f\"✓ Columns: {list(detailed_summary_generated_data.column_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Key facts Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate similar data for key facts \n",
    "flow_name = \"Key Facts Knowledge Tuning Dataset Generation Flow\"\n",
    "flow_path = FlowRegistry.get_flow_path(flow_name)\n",
    "flow = Flow.from_yaml(flow_path)\n",
    "\n",
    "# Set model configuration\n",
    "flow.set_model_config(\n",
    "    model=MODEL_NAME,\n",
    "    api_base=ENDPOINT,\n",
    "    api_key=API_KEY,\n",
    ")\n",
    "\n",
    "runtime_params = {}\n",
    "\n",
    "if ENABLE_REASONING:\n",
    "    # Increase max tokens for Question Generation to accommodate reasoning content\n",
    "    runtime_params = {\n",
    "        'generate_key_fact_qa': {'max_tokens': 6000}, \n",
    "        }\n",
    "\n",
    "# Generate data for key facts summary\n",
    "key_facts_generated_data = flow.generate(quality_corpus, runtime_params=runtime_params, max_concurrency=MAX_CONCURRENCY)\n",
    "\n",
    "key_facts_generated_data.to_json(os.path.join(OUTPUT_DIR, 'key_facts_to_qa', 'gen.jsonl'), orient='records', lines=True)\n",
    "\n",
    "print(f\"✓ Key facts: {len(key_facts_generated_data)} records\")\n",
    "\n",
    "print(f\"✓ Columns: {list(key_facts_generated_data.column_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Document Based Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_name = \"Document Based Knowledge Tuning Dataset Generation Flow\"\n",
    "flow_path = FlowRegistry.get_flow_path(flow_name)\n",
    "flow = Flow.from_yaml(flow_path)\n",
    "\n",
    "# Set model configuration\n",
    "flow.set_model_config(\n",
    "    model=MODEL_NAME,\n",
    "    api_base=ENDPOINT,\n",
    "    api_key=API_KEY,\n",
    ")\n",
    "\n",
    "runtime_params = {}\n",
    "\n",
    "if ENABLE_REASONING:\n",
    "    # Increase max tokens to accommodate reasoning content\n",
    "    runtime_params = {\n",
    "        'question_generation': {'max_tokens': 2048}, \n",
    "        }\n",
    "\n",
    "document_based_generated_data = flow.generate(quality_corpus, runtime_params=runtime_params, max_concurrency=MAX_CONCURRENCY)\n",
    "    \n",
    "document_based_generated_data.to_json(os.path.join(OUTPUT_DIR, 'document_based_qa', 'gen.jsonl'), orient='records', lines=True)\n",
    "\n",
    "print(f\"✓ Document based: {len(document_based_generated_data)} records\")\n",
    "\n",
    "print(f\"✓ Columns: {list(document_based_generated_data.column_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🎉 You now have all three four of document augmentations (detailed summaries, extractive summaries, key facts and document based) along with their corresponding QA pairs.\n",
    "\n",
    "✅ Next steps:\n",
    "   - Combine and curate these datasets to prepare your final training data.\n",
    "   - For detailed guidance on post-processing, mixing, and formatting the data for model training (including conversion to messages format), please refer to [knowledge_mixing.ipynb](knowledge_mixing.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "02-knowledge-generation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
