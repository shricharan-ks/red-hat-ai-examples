{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1289e284",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/scharan/projects/red-hat-ai-examples/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6970dad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKSPACE = Path.cwd().parent\n",
    "\n",
    "OUTPUT_DIR = WORKSPACE / \"output\" / \"step_03\"\n",
    "\n",
    "SDG_OUTPUT_DIR = WORKSPACE / \"output\"  / \"step_02\"\n",
    "\n",
    "BASE_MODEL_NAME =  \"meta-llama/Llama-3.2-1B-Instruct\" #\"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "BASE_MODEL_PATH = OUTPUT_DIR / \"base_model\" / BASE_MODEL_NAME.split(\"/\")[-1]\n",
    "\n",
    "import os \n",
    "\n",
    "hf_token= \"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82f043b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa16b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model meta-llama/Llama-3.2-1B-Instruct\n",
      "Model saved to /Users/scharan/projects/red-hat-ai-examples/examples/knowledge-tuning/output/step_03/base_model/Llama-3.2-1B-Instruct\n",
      "Loading tokenizer meta-llama/Llama-3.2-1B-Instruct\n",
      "Tokenizer saved to /Users/scharan/projects/red-hat-ai-examples/examples/knowledge-tuning/output/step_03/base_model/Llama-3.2-1B-Instruct\n"
     ]
    }
   ],
   "source": [
    "if not BASE_MODEL_PATH.exists():\n",
    "    print(\"Model not available locally, Downloading the model locally \")\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "    \n",
    "    # Save the model\n",
    "    print(f\"Loading model {BASE_MODEL_NAME}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_NAME)\n",
    "    model.save_pretrained(BASE_MODEL_PATH)\n",
    "    print(f\"Model saved to {BASE_MODEL_PATH}\")\n",
    "    \n",
    "    \n",
    "    # Save the tokenizer\n",
    "    print(f\"Loading tokenizer {BASE_MODEL_NAME}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
    "    tokenizer.save_pretrained(BASE_MODEL_PATH)\n",
    "    print(f\"Tokenizer saved to {BASE_MODEL_PATH}\")\n",
    "else:\n",
    "    print(f\"Model Available locally : {BASE_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78c61ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã OSFT Multi-Phase Configuration\n",
      "==================================================\n",
      "Base Model: /Users/scharan/projects/red-hat-ai-examples/examples/knowledge-tuning/output/step_03/base_model/Llama-3.2-1B-Instruct\n",
      "Phase07 Data: /Users/scharan/projects/red-hat-ai-examples/examples/knowledge-tuning/output/step_02/instructlab_phase_1_ds.jsonl\n",
      "Phase10 Data: /Users/scharan/projects/red-hat-ai-examples/examples/knowledge-tuning/output/step_02/instructlab_phase_2_ds.jsonl\n",
      "Output Directory: /Users/scharan/projects/red-hat-ai-examples/examples/knowledge-tuning/output/step_03/checkpoints/osft_multiphase_experiment_20250925_123208\n",
      "\n",
      "‚ú® Key Difference from Traditional LAB SFT:\n",
      "  Phase10 only needs skills data - no replay buffers!\n",
      "  OSFT preserves Phase07 knowledge automatically.\n",
      "  This workflow replaces complex LAB multi-phase training.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MODEL AND DATA CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# Data paths for each phase\n",
    "PHASE07_DATA_PATH = SDG_OUTPUT_DIR / \"instructlab_phase_1_ds.jsonl\" # Knowledge data\n",
    "PHASE10_DATA_PATH = SDG_OUTPUT_DIR / \"instructlab_phase_2_ds.jsonl\"   # Skills data ONLY (no replay needed!)\n",
    "\n",
    "# Output configuration\n",
    "CHECKPOINT_BASE_DIR = OUTPUT_DIR / \"checkpoints\"\n",
    "EXPERIMENT_PREFIX = \"osft_multiphase_experiment\"\n",
    "\n",
    "# Create timestamped experiment directory\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "experiment_name = f\"{EXPERIMENT_PREFIX}_{timestamp}\"\n",
    "\n",
    "print(\"üìã OSFT Multi-Phase Configuration\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Base Model: {BASE_MODEL_PATH}\")\n",
    "print(f\"Phase07 Data: {PHASE07_DATA_PATH}\")\n",
    "print(f\"Phase10 Data: {PHASE10_DATA_PATH}\")\n",
    "print(f\"Output Directory: {CHECKPOINT_BASE_DIR}/{experiment_name}\")\n",
    "print()\n",
    "print(\"‚ú® Key Difference from Traditional LAB SFT:\")\n",
    "print(\"  Phase10 only needs skills data - no replay buffers!\")\n",
    "print(\"  OSFT preserves Phase07 knowledge automatically.\")\n",
    "print(\"  This workflow replaces complex LAB multi-phase training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16ceb0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ OSFT Progressive Unfreeze Strategy\n",
      "==================================================\n",
      "Phase07 (Knowledge): unfreeze_rank_ratio = 0.3\n",
      "Phase10 (Skills):    unfreeze_rank_ratio = 0.19999999999999998\n",
      "Reduction:           -0.1 per phase\n",
      "\n",
      "üìä Strategy Explanation:\n",
      "  ‚Ä¢ Phase07 (0.3): More freedom to acquire new knowledge\n",
      "  ‚Ä¢ Phase10 (0.19999999999999998): Reduced to preserve Phase07 learning\n",
      "\n",
      "üí° Guidelines:\n",
      "  ‚Ä¢ Start with 0.25-0.35 for Phase07\n",
      "  ‚Ä¢ Reduce by 0.05-0.15 for each subsequent phase\n",
      "  ‚Ä¢ Never go below 0.1 (too restrictive)\n",
      "  ‚Ä¢ Adjust based on your preservation needs\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# OSFT PROGRESSIVE UNFREEZE STRATEGY\n",
    "# =============================================================================\n",
    "\n",
    "# Phase07: Initial knowledge acquisition\n",
    "PHASE07_UNFREEZE_RATIO = 0.3  # Standard ratio for knowledge learning\n",
    "\n",
    "# Phase10: Reduced ratio for better preservation\n",
    "UNFREEZE_REDUCTION = 0.1  # Reduce by 10% for each subsequent phase\n",
    "PHASE10_UNFREEZE_RATIO = max(0.1, PHASE07_UNFREEZE_RATIO - UNFREEZE_REDUCTION)\n",
    "\n",
    "print(\"üéØ OSFT Progressive Unfreeze Strategy\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Phase07 (Knowledge): unfreeze_rank_ratio = {PHASE07_UNFREEZE_RATIO}\")\n",
    "print(f\"Phase10 (Skills):    unfreeze_rank_ratio = {PHASE10_UNFREEZE_RATIO}\")\n",
    "print(f\"Reduction:           -{UNFREEZE_REDUCTION} per phase\")\n",
    "print()\n",
    "print(\"üìä Strategy Explanation:\")\n",
    "print(f\"  ‚Ä¢ Phase07 ({PHASE07_UNFREEZE_RATIO}): More freedom to acquire new knowledge\")\n",
    "print(f\"  ‚Ä¢ Phase10 ({PHASE10_UNFREEZE_RATIO}): Reduced to preserve Phase07 learning\")\n",
    "print()\n",
    "print(\"üí° Guidelines:\")\n",
    "print(\"  ‚Ä¢ Start with 0.25-0.35 for Phase07\")\n",
    "print(\"  ‚Ä¢ Reduce by 0.05-0.15 for each subsequent phase\")\n",
    "print(\"  ‚Ä¢ Never go below 0.1 (too restrictive)\")\n",
    "print(\"  ‚Ä¢ Adjust based on your preservation needs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ae5865d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è  Training Hyperparameters\n",
      "==================================================\n",
      "Max Sequence Length: 8,192\n",
      "Max Tokens per GPU: 10,000\n",
      "Epochs per Phase: 2\n",
      "\n",
      "Phase07 (Knowledge):\n",
      "  ‚Ä¢ Batch Size: 128\n",
      "  ‚Ä¢ Learning Rate: 5e-06\n",
      "  ‚Ä¢ Warmup Steps: 0\n",
      "\n",
      "Phase10 (Skills):\n",
      "  ‚Ä¢ Batch Size: 128\n",
      "  ‚Ä¢ Learning Rate: 5e-06 (reduced for preservation)\n",
      "  ‚Ä¢ Warmup Steps: 0\n",
      "\n",
      "Distributed: 8 GPUs √ó 1 nodes = 8 total GPUs\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# TRAINING HYPERPARAMETERS\n",
    "# =============================================================================\n",
    "\n",
    "# Common parameters for both phases\n",
    "MAX_SEQ_LEN = 8_192                 # Maximum sequence length\n",
    "MAX_TOKENS_PER_GPU = 10_000         # Memory limit per GPU\n",
    "NUM_EPOCHS = 2                      # Training epochs per phase\n",
    "WARMUP_STEPS = 0                    # Warmup for Phase07\n",
    "USE_LIGER = True                    # Enable Liger kernels for efficiency\n",
    "\n",
    "# Phase07 specific parameters\n",
    "PHASE07_BATCH_SIZE = 128            # Batch size for knowledge training\n",
    "PHASE07_LEARNING_RATE = 5e-6        # Use low learning rate for better learning quality\n",
    "\n",
    "# Phase10 specific parameters  \n",
    "PHASE10_BATCH_SIZE = 128            # Can use same batch size (no replay data!)\n",
    "PHASE10_LEARNING_RATE = 5e-6        # Use low learning rate for better learning quality\n",
    "PHASE10_WARMUP_STEPS = 0            # No warmup\n",
    "\n",
    "# Distributed training configuration\n",
    "NPROC_PER_NODE = 8                  # Number of GPUs per node\n",
    "NNODES = 1                          # Number of nodes\n",
    "NODE_RANK = 0                       # Rank of this node\n",
    "RDZV_ID = 47                        # Unique job ID\n",
    "RDZV_ENDPOINT = \"127.0.0.1:29500\"   # Rendezvous endpoint\n",
    "\n",
    "print(\"‚öôÔ∏è  Training Hyperparameters\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Max Sequence Length: {MAX_SEQ_LEN:,}\")\n",
    "print(f\"Max Tokens per GPU: {MAX_TOKENS_PER_GPU:,}\")\n",
    "print(f\"Epochs per Phase: {NUM_EPOCHS}\")\n",
    "print()\n",
    "print(\"Phase07 (Knowledge):\")\n",
    "print(f\"  ‚Ä¢ Batch Size: {PHASE07_BATCH_SIZE}\")\n",
    "print(f\"  ‚Ä¢ Learning Rate: {PHASE07_LEARNING_RATE}\")\n",
    "print(f\"  ‚Ä¢ Warmup Steps: {WARMUP_STEPS}\")\n",
    "print()\n",
    "print(\"Phase10 (Skills):\")\n",
    "print(f\"  ‚Ä¢ Batch Size: {PHASE10_BATCH_SIZE}\")\n",
    "print(f\"  ‚Ä¢ Learning Rate: {PHASE10_LEARNING_RATE} (reduced for preservation)\")\n",
    "print(f\"  ‚Ä¢ Warmup Steps: {PHASE10_WARMUP_STEPS}\")\n",
    "print()\n",
    "print(f\"Distributed: {NPROC_PER_NODE} GPUs √ó {NNODES} nodes = {NPROC_PER_NODE * NNODES} total GPUs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cd1562b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import redirect_stdout, redirect_stderr\n",
    "from io import StringIO\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29a3ff33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Checkpoint utility functions defined\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "def find_most_recent_checkpoint(output_dir):\n",
    "    \"\"\"\n",
    "    Find the most recent checkpoint in the training output directory.\n",
    "    \n",
    "    Args:\n",
    "        output_dir (str): Training output directory containing hf_format/ subdirectory\n",
    "        \n",
    "    Returns:\n",
    "        str: Path to the most recent checkpoint\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If no checkpoints are found\n",
    "    \"\"\"\n",
    "    # Get all checkpoint directories under hf_format\n",
    "    checkpoint_pattern = os.path.join(output_dir, \"hf_format\", \"samples_*.0\")\n",
    "    checkpoint_dirs = glob.glob(checkpoint_pattern)\n",
    "    \n",
    "    if not checkpoint_dirs:\n",
    "        raise ValueError(f\"No checkpoints found in {os.path.join(output_dir, 'hf_format')}\")\n",
    "    \n",
    "    # Find the most recently created checkpoint\n",
    "    most_recent_checkpoint = max(checkpoint_dirs, key=os.path.getctime)\n",
    "    \n",
    "    return most_recent_checkpoint\n",
    "\n",
    "print(\"‚úÖ Checkpoint utility functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0570ce9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from training_hub import osft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "658ca243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Phase 1: Knowledge Training with OSFT\n",
      "============================================================\n",
      "Starting from: /Users/scharan/projects/red-hat-ai-examples/examples/knowledge-tuning/output/step_03/base_model/Llama-3.2-1B-Instruct\n",
      "Training data: /Users/scharan/projects/red-hat-ai-examples/examples/knowledge-tuning/output/step_02/instructlab_phase_1_ds.jsonl\n",
      "Output directory: /Users/scharan/projects/red-hat-ai-examples/examples/knowledge-tuning/output/step_03/checkpoints/osft_multiphase_experiment_20250925_123208/phase07_knowledge_training\n",
      "Unfreeze ratio: 0.3\n",
      "\n",
      "‚ùå Phase07 training failed: Training failed. Please check the logs at /Users/scharan/projects/red-hat-ai-examples/examples/knowledge-tuning/output/step_03/checkpoints/osft_multiphase_experiment_20250925_123208/phase07_knowledge_training/training_log_node0.log for details.\n",
      "\n",
      "Error details:\n",
      "Generating train split: 53 examples [00:00, 3444.88 examples/s]\n",
      "Ensuring dataset is compatible with legacy format. (num_proc=8): 100%|##########| 53/53 [00:00<00:00, 310.40 examples/s]\n",
      "Converting samples into input_ids and labels... (num_proc=8): 100%|##########| 53/53 [00:02<00:00, 22.61 examples/s]\n",
      "Filter (num_proc=8): 100%|##########| 53/53 [00:00<00:00, 466.23 examples/s]\n",
      "Filtering out pretraining samples (num_proc=8): 100%|##########| 53/53 [00:00<00:00, 464.87 examples/s]\n",
      "Filtering out pretraining samples (num_proc=8): 100%|##########| 53/53 [00:00<00:00, 504.19 examples/s]\n",
      "Validating unmask tokens not in data (num_proc=8): 100%|##########| 53/53 [00:00<00:00, 517.53 examples/s]\n",
      "Creating json from Arrow format: 100%|##########| 1/1 [00:01<00:00,  1.03s/ba]\n",
      "2025-09-25 12:33:21,868 - mini_trainer.api_train - INFO - Starting training setup...\n",
      "2025-09-25 12:33:21,869 - mini_trainer.api_train - INFO - Running training command as subprocess: torchrun --nnodes=1 --node_rank=0 --nproc_per_node=8 --rdzv_id=47 --rdzv_endpoint=127.0.0.1:29500 /Users/scharan/projects/red-hat-ai-examples/.venv/lib/python3.12/site-packages/mini_trainer/train.py --model-name-or-path=/Users/scharan/projects/red-hat-ai-examples/examples/knowledge-tuning/output/step_03/base_model/Llama-3.2-1B-Instruct --data-path=/Users/scharan/projects/red-hat-ai-examples/examples/knowledge-tuning/output/step_03/checkpoints/osft_multiphase_experiment_20250925_123208/phase07_knowledge_training/data_processing/data.jsonl --batch-size=128 --max-tokens-per-gpu=10000 --learning-rate=5e-06 --num-warmup-steps=0 --lr-scheduler=cosine --lr-scheduler-kwargs={} --seed=42 --output-dir=/Users/scharan/projects/red-hat-ai-examples/examples/knowledge-tuning/output/step_03/checkpoints/osft_multiphase_experiment_20250925_123208/phase07_knowledge_training --training-mode=epoch --max-epochs=2 --max-steps=0 --max-tokens=0 --use-liger-kernels --osft --osft-unfreeze-rank-ratio=0.3 --checkpoint-at-epoch --save-final-checkpoint\n",
      "2025-09-25 12:33:34,707 - mini_trainer.api_train - ERROR - Training subprocess failed. Check logs for details.\n",
      "2025-09-25 12:33:34,708 - mini_trainer.api_train - INFO - Waiting for process to exit (60s timeout)...\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Training failed. Please check the logs at /Users/scharan/projects/red-hat-ai-examples/examples/knowledge-tuning/output/step_03/checkpoints/osft_multiphase_experiment_20250925_123208/phase07_knowledge_training/training_log_node0.log for details.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m redirect_stdout(output_buffer), redirect_stderr(error_buffer):\n\u001b[32m     23\u001b[39m         \u001b[38;5;66;03m# Phase07 OSFT training\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m         phase07_result = \u001b[43mosft\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# Model and data\u001b[39;49;00m\n\u001b[32m     26\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mBASE_MODEL_PATH\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mPHASE07_DATA_PATH\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m            \u001b[49m\u001b[43mckpt_output_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mphase07_output_dir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \n\u001b[32m     30\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# OSFT-specific\u001b[39;49;00m\n\u001b[32m     31\u001b[39m \u001b[43m            \u001b[49m\u001b[43munfreeze_rank_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPHASE07_UNFREEZE_RATIO\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \n\u001b[32m     33\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# Training parameters\u001b[39;49;00m\n\u001b[32m     34\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m            \u001b[49m\u001b[43meffective_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPHASE07_BATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPHASE07_LEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMAX_SEQ_LEN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmax_tokens_per_gpu\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMAX_TOKENS_PER_GPU\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \n\u001b[32m     40\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# Data processing\u001b[39;49;00m\n\u001b[32m     41\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdata_output_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mphase07_output_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata_processing\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m            \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mWARMUP_STEPS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \n\u001b[32m     44\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# Optimization\u001b[39;49;00m\n\u001b[32m     45\u001b[39m \u001b[43m            \u001b[49m\u001b[43muse_liger\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUSE_LIGER\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m            \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcosine\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \n\u001b[32m     49\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# Checkpointing\u001b[39;49;00m\n\u001b[32m     50\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcheckpoint_at_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m            \u001b[49m\u001b[43msave_final_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \n\u001b[32m     53\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# Distributed training\u001b[39;49;00m\n\u001b[32m     54\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnproc_per_node\u001b[49m\u001b[43m=\u001b[49m\u001b[43mNPROC_PER_NODE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnnodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mNNODES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnode_rank\u001b[49m\u001b[43m=\u001b[49m\u001b[43mNODE_RANK\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrdzv_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mRDZV_ID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrdzv_endpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mRDZV_ENDPOINT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m     phase07_duration = time.time() - phase07_start_time\n\u001b[32m     63\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Phase07 completed successfully in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mphase07_duration/\u001b[32m3600\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m hours!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/red-hat-ai-examples/.venv/lib/python3.12/site-packages/training_hub/algorithms/osft.py:473\u001b[39m, in \u001b[36mosft\u001b[39m\u001b[34m(model_path, data_path, unfreeze_rank_ratio, effective_batch_size, max_tokens_per_gpu, max_seq_len, learning_rate, ckpt_output_dir, data_output_dir, backend, target_patterns, seed, use_liger, use_processed_dataset, unmask_messages, lr_scheduler, warmup_steps, lr_scheduler_kwargs, checkpoint_at_epoch, save_final_checkpoint, num_epochs, nproc_per_node, nnodes, node_rank, rdzv_id, rdzv_endpoint, **kwargs)\u001b[39m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_algorithm\n\u001b[32m    472\u001b[39m algorithm = create_algorithm(\u001b[33m'\u001b[39m\u001b[33mosft\u001b[39m\u001b[33m'\u001b[39m, backend)\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m    \u001b[49m\u001b[43mckpt_output_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_output_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_output_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_output_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m    \u001b[49m\u001b[43munfreeze_rank_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[43munfreeze_rank_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m    \u001b[49m\u001b[43meffective_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43meffective_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_tokens_per_gpu\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_tokens_per_gpu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget_patterns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget_patterns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_liger\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_liger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_processed_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_processed_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m    \u001b[49m\u001b[43munmask_messages\u001b[49m\u001b[43m=\u001b[49m\u001b[43munmask_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr_scheduler_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr_scheduler_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_at_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheckpoint_at_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_final_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_final_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnproc_per_node\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnproc_per_node\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnnodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnode_rank\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnode_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrdzv_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrdzv_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrdzv_endpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrdzv_endpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/red-hat-ai-examples/.venv/lib/python3.12/site-packages/training_hub/algorithms/osft.py:195\u001b[39m, in \u001b[36mOSFTAlgorithm.train\u001b[39m\u001b[34m(self, model_path, data_path, unfreeze_rank_ratio, effective_batch_size, max_tokens_per_gpu, max_seq_len, learning_rate, ckpt_output_dir, target_patterns, seed, use_liger, lr_scheduler, warmup_steps, lr_scheduler_kwargs, checkpoint_at_epoch, save_final_checkpoint, num_epochs, use_processed_dataset, unmask_messages, data_output_dir, nproc_per_node, nnodes, node_rank, rdzv_id, rdzv_endpoint, **kwargs)\u001b[39m\n\u001b[32m    192\u001b[39m all_params.update(optional_params)\n\u001b[32m    193\u001b[39m all_params.update(kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/red-hat-ai-examples/.venv/lib/python3.12/site-packages/training_hub/algorithms/osft.py:381\u001b[39m, in \u001b[36mMiniTrainerOSFTBackend.execute_training\u001b[39m\u001b[34m(self, algorithm_params)\u001b[39m\n\u001b[32m    377\u001b[39m torchrun_args_pre[\u001b[33m'\u001b[39m\u001b[33mrdzv_endpoint\u001b[39m\u001b[33m'\u001b[39m] = torchrun_args_pre.get(\u001b[33m'\u001b[39m\u001b[33mrdzv_endpoint\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mlocalhost:1738\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    380\u001b[39m \u001b[38;5;66;03m# now we run training\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m381\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTorchrunArgs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtorchrun_args_pre\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTrainingArgs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtraining_args_pre\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/red-hat-ai-examples/.venv/lib/python3.12/site-packages/mini_trainer/api_train.py:178\u001b[39m, in \u001b[36mrun_training\u001b[39m\u001b[34m(torch_args, train_args)\u001b[39m\n\u001b[32m    176\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m interrupt\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m failure:\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    179\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining failed. Please check the logs at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlog_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for details.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    180\u001b[39m     )\n",
      "\u001b[31mRuntimeError\u001b[39m: Training failed. Please check the logs at /Users/scharan/projects/red-hat-ai-examples/examples/knowledge-tuning/output/step_03/checkpoints/osft_multiphase_experiment_20250925_123208/phase07_knowledge_training/training_log_node0.log for details."
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PHASE 1 (PHASE07): KNOWLEDGE TRAINING\n",
    "# =============================================================================\n",
    "\n",
    "phase07_output_dir = CHECKPOINT_BASE_DIR / experiment_name / \"phase07_knowledge_training\"\n",
    "\n",
    "print(\"üìö Phase 1: Knowledge Training with OSFT\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Starting from: {BASE_MODEL_PATH}\")\n",
    "print(f\"Training data: {PHASE07_DATA_PATH}\")\n",
    "print(f\"Output directory: {phase07_output_dir}\")\n",
    "print(f\"Unfreeze ratio: {PHASE07_UNFREEZE_RATIO}\")\n",
    "print()\n",
    "\n",
    "# Capture output to prevent notebook crashes\n",
    "output_buffer = StringIO()\n",
    "error_buffer = StringIO()\n",
    "\n",
    "phase07_start_time = time.time()\n",
    "\n",
    "try:\n",
    "    with redirect_stdout(output_buffer), redirect_stderr(error_buffer):\n",
    "        # Phase07 OSFT training\n",
    "        phase07_result = osft(\n",
    "            # Model and data\n",
    "            model_path=f\"{BASE_MODEL_PATH}\",\n",
    "            data_path=f\"{PHASE07_DATA_PATH}\",\n",
    "            ckpt_output_dir=f\"{phase07_output_dir}\",\n",
    "            \n",
    "            # OSFT-specific\n",
    "            unfreeze_rank_ratio=PHASE07_UNFREEZE_RATIO,\n",
    "            \n",
    "            # Training parameters\n",
    "            num_epochs=NUM_EPOCHS,\n",
    "            effective_batch_size=PHASE07_BATCH_SIZE,\n",
    "            learning_rate=PHASE07_LEARNING_RATE,\n",
    "            max_seq_len=MAX_SEQ_LEN,\n",
    "            max_tokens_per_gpu=MAX_TOKENS_PER_GPU,\n",
    "            \n",
    "            # Data processing\n",
    "            data_output_dir=os.path.join(phase07_output_dir, \"data_processing\"),\n",
    "            warmup_steps=WARMUP_STEPS,\n",
    "            \n",
    "            # Optimization\n",
    "            use_liger=USE_LIGER,\n",
    "            seed=42,\n",
    "            lr_scheduler=\"cosine\",\n",
    "            \n",
    "            # Checkpointing\n",
    "            checkpoint_at_epoch=True,\n",
    "            save_final_checkpoint=True,\n",
    "            \n",
    "            # Distributed training\n",
    "            nproc_per_node=NPROC_PER_NODE,\n",
    "            nnodes=NNODES,\n",
    "            node_rank=NODE_RANK,\n",
    "            rdzv_id=RDZV_ID,\n",
    "            rdzv_endpoint=RDZV_ENDPOINT,\n",
    "        )\n",
    "    \n",
    "    phase07_duration = time.time() - phase07_start_time\n",
    "    \n",
    "    print(f\"‚úÖ Phase07 completed successfully in {phase07_duration/3600:.2f} hours!\")\n",
    "    print(f\"üìÅ Checkpoint saved to: {phase07_output_dir}\")\n",
    "    print()\n",
    "    print(\"üìä Phase07 Achievements:\")\n",
    "    print(\"  ‚Ä¢ Base model capabilities: ‚úÖ Preserved\")\n",
    "    print(\"  ‚Ä¢ New knowledge integrated: ‚úÖ Complete\")\n",
    "    print(\"  ‚Ä¢ Ready for Phase10: ‚úÖ Yes\")\n",
    "    \n",
    "    # Find the most recent checkpoint for Phase10\n",
    "    PHASE07_CHECKPOINT = find_most_recent_checkpoint(phase07_output_dir)\n",
    "    print(f\"üìÅ Found most recent Phase07 checkpoint: {PHASE07_CHECKPOINT}\")\n",
    "    print(f\"üìÅ Ready for Phase10 training!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Phase07 training failed: {e}\")\n",
    "    print(\"\\nError details:\")\n",
    "    print(error_buffer.getvalue())\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be213e37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-utils",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
