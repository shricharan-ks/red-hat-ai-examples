{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1dfca11",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Evaluation is a crucial step in model training, allowing us to measure the performance and generalization ability of our models. In this notebook, we will systematically evaluate both the base model and the finetuned model using appropriate metrics and validation datasets.\n",
    "\n",
    "- **Base Model Evaluation:**  \n",
    "    We will assess the initial performance of the base model to establish a benchmark. This helps us understand how well the model performs before any task-specific adaptation.\n",
    "\n",
    "- **Finetuned Model Evaluation:**  \n",
    "    After training the model on our specific dataset, we will evaluate its performance again. Comparing these results with the base model will highlight the improvements gained through finetuning.\n",
    "\n",
    "Throughout this notebook, we will use visualizations and quantitative metrics to provide a comprehensive analysis of model performance. This approach ensures transparency and helps guide further improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c323ab",
   "metadata": {},
   "source": [
    "## Setup Paths and Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6129be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from rich import print\n",
    "\n",
    "\n",
    "WORKSPACE = Path.cwd().parent  # Path to the workspace directory\n",
    "\n",
    "OUTPUT_DIR = WORKSPACE / \"output\" / \"step_04\"\n",
    "\n",
    "OUTPUT_DIR.mkdir(\n",
    "    parents=True, exist_ok=True\n",
    ")  # Create output directory if it doesn't exist\n",
    "\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "TRAINING_OUTPUT_PATH = WORKSPACE / \"output\" / \"step_04\"\n",
    "\n",
    "BASE_MODEL_PATH = TRAINING_OUTPUT_PATH / \"base_model\" / MODEL_NAME.split(\"/\")[-1]\n",
    "\n",
    "FINE_TUNED_MODEL_PATH = TRAINING_OUTPUT_PATH / \"fine_tuned_model\" / MODEL_NAME.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dc2db5",
   "metadata": {},
   "source": [
    "## Load the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77fe897",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_PATH,\n",
    "    dtype=torch.float16,\n",
    "    device_map=\"cuda:0\",\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "base_model_tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)\n",
    "\n",
    "\n",
    "\n",
    "fine_tuned_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_PATH,\n",
    "    dtype=torch.float16,\n",
    "    device_map=\"cuda:0\",\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "fine_tuned_model_tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f1d66b",
   "metadata": {},
   "source": [
    "## LLM Sampling Parameters\n",
    "\n",
    "We will use the below when testing our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3abfb288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX_NEW_TOKENS: 256\n",
      "DO_SAMPLE: True\n",
      "TEMPERATURE: 0.7\n",
      "TOP_P: 0.9\n",
      "âœ… LLM sampling parameters defined\n",
      "\n",
      "ðŸ“Š Using Meta's recommended Llama sampling settings:\n",
      "  â€¢ Temperature 0.6 for balanced creativity/consistency\n",
      "  â€¢ Top-p 0.9 for good token diversity\n",
      "  â€¢ Stop on both EOS and <|eot_id|> tokens\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# ðŸŽ¯ Sampling/Generation Parameters                                            #\n",
    "################################################################################\n",
    "MAX_NEW_TOKENS = 256\n",
    "DO_SAMPLE = True\n",
    "TEMPERATURE = 0.7  # Meta's recommended temperature for Llama\n",
    "TOP_P = 0.9        # Standard top_p for Llama models\n",
    "\n",
    "print(f\"MAX_NEW_TOKENS: {MAX_NEW_TOKENS}\")\n",
    "print(f\"DO_SAMPLE: {DO_SAMPLE}\")\n",
    "print(f\"TEMPERATURE: {TEMPERATURE}\")\n",
    "print(f\"TOP_P: {TOP_P}\")\n",
    "print(\"âœ… LLM sampling parameters defined\")\n",
    "print()\n",
    "print(\"ðŸ“Š Using Meta's recommended Llama sampling settings:\")\n",
    "print(\"  â€¢ Temperature 0.6 for balanced creativity/consistency\")\n",
    "print(\"  â€¢ Top-p 0.9 for good token diversity\")\n",
    "print(\"  â€¢ Stop on both EOS and <|eot_id|> tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144e4629",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca6785d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_runner(model,tokenizer, prompt):\n",
    "    inputs = tokenizer(prompt,return_tensors='pt').to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            temperature=TEMPERATURE,\n",
    "            do_sample=DO_SAMPLE,\n",
    "            top_p=TOP_P,\n",
    "        )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:],skip_special_tokens=True)\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "def run_experimentation(prompt):\n",
    "    # Run prompt in base model\n",
    "    base_model_response = prompt_runner(\n",
    "        model=base_model,\n",
    "        tokenizer=base_model_tokenizer,\n",
    "        prompt=prompt)\n",
    "\n",
    "\n",
    "    # Run prompt in fine_tuned_model\n",
    "    fine_tuned_model_response = prompt_runner(\n",
    "        model=fine_tuned_model,\n",
    "        tokenizer=fine_tuned_model_tokenizer,\n",
    "        prompt=prompt)\n",
    "\n",
    "\n",
    "    # Print the response of the model\n",
    "\n",
    "    print(f\"\"\"\n",
    "    [bold]EXPERIMENTATION DETAILS[/bold]:\n",
    "        MODEL NAME     : {MODEL_NAME}\n",
    "        MAX NEW TOKENS : {MAX_NEW_TOKENS}\n",
    "        DO SAMPLE      : {DO_SAMPLE}\n",
    "        TEMPERATURE    : {TEMPERATURE}\n",
    "        TOP P          : {TOP_P}      \n",
    "\n",
    "\n",
    "    [bold]PROMPT ðŸ’¬[/bold]:\n",
    "\n",
    "        [green]{prompt}[/green]\n",
    "\n",
    "    [bold]BASE MODEL RESPONSE ðŸ¤–[/bold]:\n",
    "\n",
    "        {base_model_response}\n",
    "\n",
    "    [bold]FINE TUNED MODEL RESPONSE ðŸ¤–[/bold]:\n",
    "\n",
    "        {fine_tuned_model_response}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0d221b",
   "metadata": {},
   "source": [
    "## Test 1\n",
    "\n",
    "We will test the knowledge of the model on BMO data. \n",
    "\n",
    "Question: `what is the meaning of verifying the identity of a person or an entity`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4175cfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"what is the meaning of verifying the identity of a person or an entity\"\"\"\n",
    "\n",
    "\n",
    "run_experimentation(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "03-knowledge-mixing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
