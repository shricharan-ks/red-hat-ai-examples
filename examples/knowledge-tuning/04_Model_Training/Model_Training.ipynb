{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b63bb30",
   "metadata": {},
   "source": [
    "# OSFT Continual Learning Demo\n",
    "\n",
    "Fine-tuning language models is hard‚Äîyou need good data, lots of resources, and even small changes can cause problems. This makes it tough to add new abilities to a model. This problem is called **continual learning** and is what our new training technique, orthogonal subspace fine-tuning (OSFT), solves.\n",
    "\n",
    "This notebook presents a hands-on example where we enhance `meta-llama/Meta-Llama-3-8B-Instruct` by teaching it to only produce JSON output when requested.\n",
    "\n",
    "By the end of this notebook, you will learn:\n",
    "- ‚úÖ How Llama can be fine-tuned without destroying its existing capabilities\n",
    "- ‚úÖ How to enhance your own LLMs with OSFT\n",
    "- ‚úÖ Best practices when fine-tuning models\n",
    "- ‚ùå OSFT does NOT kill your existing model when trained on new data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e37928b",
   "metadata": {},
   "source": [
    "## Setup Paths and Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103b0cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "WORKSPACE = Path.cwd().parent  # Path to the workspace directory\n",
    "\n",
    "OUTPUT_DIR = WORKSPACE / \"output\" / \"step_04\"\n",
    "\n",
    "OUTPUT_DIR.mkdir(\n",
    "    parents=True, exist_ok=True\n",
    ")  # Create output directory if it doesn't exist\n",
    "\n",
    "KNOWLEDGE_MIXED_DATASET_PATH = WORKSPACE / \"output\" / \"step_03\" / \"training_mix\"\n",
    "\n",
    "################################################################################\n",
    "# ü§ñ Model + Data Paths                                                        #\n",
    "################################################################################\n",
    "BASE_MODEL_NAME = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "DATASET_PATH = KNOWLEDGE_MIXED_DATASET_PATH / \"combined_cut_5x.jsonl\" # Path to the Training Dataset from Step 03\n",
    "CHECKPOINTS_PATH = OUTPUT_DIR / \"checkpoint\"\n",
    "DATA_OUTPUT_PATH = OUTPUT_DIR / \"dev\" / \"shm\"  # for quicker multi-process loading of datasets\n",
    "\n",
    "\n",
    "\n",
    "BASE_MODEL_PATH = OUTPUT_DIR / \"base_model\" / BASE_MODEL_NAME.split(\"/\")[-1]\n",
    "\n",
    "\n",
    "# Authenticate to Hugging Face if required to pull your base model\n",
    "# from huggingface_hub import login\n",
    "# HF_TOKEN = \"\" # Insert your API Token\n",
    "# login(HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81dddf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE THE MODEL LOCALLY\n",
    "\n",
    "if not BASE_MODEL_PATH.exists():\n",
    "    print(\"Model not available locally, Downloading the model locally \")\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "    # Save the model\n",
    "    print(f\"Loading model {BASE_MODEL_NAME}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_NAME)\n",
    "    model.save_pretrained(BASE_MODEL_PATH)\n",
    "    print(f\"Model saved to {BASE_MODEL_PATH}\")\n",
    "\n",
    "    # Save the tokenizer\n",
    "    print(f\"Loading tokenizer {BASE_MODEL_NAME}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
    "    tokenizer.save_pretrained(BASE_MODEL_PATH)\n",
    "    print(f\"Tokenizer saved to {BASE_MODEL_PATH}\")\n",
    "else:\n",
    "    print(f\"Model Available locally : {BASE_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04e296c",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "First, let's import and configure everything that we need to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c38f218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Set these env variables so we can properly clear the memory after training\n",
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "# We have to enable this so we can properly clear the model from memory after inferencing\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:4096,expandable_segments:True\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa4ac16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import training_hub for OSFT training\n",
    "from training_hub import osft\n",
    "\n",
    "# Standard library imports\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import sys\n",
    "from contextlib import redirect_stdout, redirect_stderr\n",
    "from io import StringIO\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5842112",
   "metadata": {},
   "source": [
    "## Logging Configuration\n",
    "\n",
    "Set up logging to track progress while preventing notebook crashes from excessive output.\n",
    "<!-- \n",
    "**Note:** For production workflows or long-running jobs, we recommend using the script version at `scripts/osft-training.py` for better logging consistency and resumption capabilities.\n",
    "\n",
    "**Quick script usage:**\n",
    "```bash\n",
    "python scripts/lab_multiphase_osft_training.py \\\n",
    "  --base-model-path /path/to/model \\\n",
    "  --phase07-data-path /path/to/knowledge.jsonl \\\n",
    "  --phase10-data-path /path/to/skills.jsonl \\\n",
    "  --ckpt-output-base-dir /path/to/checkpoints -->\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4350c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging to show only essential information\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Suppress verbose logging from transformers and other libraries\n",
    "logging.getLogger(\"transformers\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"datasets\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"torch\").setLevel(logging.WARNING)\n",
    "\n",
    "print(\"‚úÖ Logging configured for notebook environment\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ae4b87",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "Let's define some helper functions for checkpoint management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1917efda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "\n",
    "def find_most_recent_checkpoint(output_dir):\n",
    "    \"\"\"\n",
    "    Find the most recent checkpoint in the training output directory.\n",
    "    \n",
    "    Args:\n",
    "        output_dir (str): Training output directory containing hf_format/ subdirectory\n",
    "        \n",
    "    Returns:\n",
    "        str: Path to the most recent checkpoint\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If no checkpoints are found\n",
    "    \"\"\"\n",
    "    # Get all checkpoint directories under hf_format\n",
    "    checkpoint_pattern = os.path.join(output_dir, \"hf_format\", \"samples_*.0\")\n",
    "    checkpoint_dirs = glob.glob(checkpoint_pattern)\n",
    "    \n",
    "    if not checkpoint_dirs:\n",
    "        raise ValueError(f\"No checkpoints found in {os.path.join(output_dir, 'hf_format')}\")\n",
    "    \n",
    "    # Find the most recently created checkpoint\n",
    "    most_recent_checkpoint = max(checkpoint_dirs, key=os.path.getctime)\n",
    "    \n",
    "    return most_recent_checkpoint\n",
    "\n",
    "def save_best_model(source_directory):\n",
    "    \"\"\"This function copies the final model from the checkpoints dir to the base output dir for easier access.\n",
    "\n",
    "    Args:\n",
    "        source_directory: Path to the recent checkpoint\n",
    "    \"\"\"\n",
    "    FINAL_FINE_TUNED_MODEL_PATH = OUTPUT_DIR / \"fine_tuned_model\" / BASE_MODEL_NAME.split('/')[-1]\n",
    "    FINAL_FINE_TUNED_MODEL_PATH.mkdir(exist_ok=True,parents=True) # Create the directory if not available\n",
    "\n",
    "    # Iterate through all files/folders in source and copy them\n",
    "    for item in os.listdir(source_directory):\n",
    "        src_path = os.path.join(source_directory, item)\n",
    "        dst_path = os.path.join(FINAL_FINE_TUNED_MODEL_PATH, item)\n",
    "\n",
    "        # Copy directories or files appropriately\n",
    "        if os.path.isdir(src_path):\n",
    "            shutil.copytree(src_path, dst_path, dirs_exist_ok=True)\n",
    "        else:\n",
    "            shutil.copy2(src_path, dst_path)\n",
    "\n",
    "    print(f\"‚úÖ Final finetuned model copied\\n\\t Path :{FINAL_FINE_TUNED_MODEL_PATH}\")\n",
    "    return FINAL_FINE_TUNED_MODEL_PATH\n",
    "\n",
    "def cleanup_model_memory(*objects):\n",
    "    \"\"\"\n",
    "    Clean up GPU memory by deleting arbitrary objects and clearing CUDA cache.\n",
    "    \n",
    "    Args:\n",
    "        *objects: Variable number of objects to clean up (models, tokenizers, etc.)\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    import gc\n",
    "    \n",
    "    # Delete all provided objects\n",
    "    # Delete objects from global namespace if they exist there\n",
    "    for obj in objects:\n",
    "        if obj is not None:\n",
    "            # Find the variable name in globals that references this object\n",
    "            for var_name, var_obj in list(globals().items()):\n",
    "                if var_obj is obj:\n",
    "                    print(f\"üóëÔ∏è Deleting global variable: {var_name}\")\n",
    "                    del globals()[var_name]\n",
    "                    break\n",
    "            # Also delete the local reference\n",
    "            del obj\n",
    "    \n",
    "    # Force garbage collection\n",
    "    gc.collect()\n",
    "    \n",
    "    # Clear CUDA cache if CUDA is available\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    print(\"‚úÖ Model memory cleaned up and CUDA cache cleared\")\n",
    "\n",
    "\n",
    "print(\"‚úÖ Checkpoint utility functions defined\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c791b6c4",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "For this example, I will be running training on an 8xA100 box, but these hyperparameters can be adjusted for any machine; provided it is capable of running OSFT.\n",
    "\n",
    "\n",
    "üö® Ensure you have configured the number of GPUs available on the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619de35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# üèãÔ∏è‚Äç‚ôÄÔ∏è Training Hyperparameters                                                  #\n",
    "################################################################################\n",
    "# Important for OSFT\n",
    "UNFREEZE_RANK_RATIO = 0.25 \n",
    "\n",
    "# Standard parameters\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 5e-6\n",
    "NUM_EPOCHS=2\n",
    "LR_SCHEDULER=\"cosine\"\n",
    "WARMUP_STEPS=0\n",
    "SEED=42\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# üèéÔ∏è Performance Hyperparameters                                               #\n",
    "################################################################################\n",
    "USE_LIGER = True\n",
    "MAX_TOKENS_PER_GPU=10_000\n",
    "MAX_SEQ_LEN=8192\n",
    "\n",
    "################################################################################\n",
    "# üíæ Checkpointing Settings                                                    #\n",
    "################################################################################\n",
    "# Here we only want to save the very last checkpoint\n",
    "SAVE_FINAL_CHECKPOINT = True\n",
    "CHECKPOINT_AT_EPOCH = False \n",
    "\n",
    "################################################################################\n",
    "# üî• TORCHRUN SETTINGS                                                         #\n",
    "################################################################################\n",
    "NUM_GPUS=8\n",
    "NUM_NODES=1\n",
    "NODE_RANK=0\n",
    "RDZV_ID=23\n",
    "RDZV_ENDPOINT='localhost:1738'\n",
    "\n",
    "\n",
    "print(\"‚öôÔ∏è  Training Hyperparameters\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Base Model: {BASE_MODEL_NAME}\")\n",
    "print(f\"Dataset Path: {DATASET_PATH}\")\n",
    "print(f\"Checkpoints Path: {CHECKPOINTS_PATH}\")\n",
    "print(f\"Data Output Path: {DATA_OUTPUT_PATH}\")\n",
    "print()\n",
    "print(f\"Unfreeze Rank Ratio: {UNFREEZE_RANK_RATIO}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"Number of Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"LR Scheduler: {LR_SCHEDULER}\")\n",
    "print(f\"Warmup Steps: {WARMUP_STEPS}\")\n",
    "print(f\"Seed: {SEED}\")\n",
    "print()\n",
    "print(f\"Use Liger: {USE_LIGER}\")\n",
    "print(f\"Max Tokens per GPU: {MAX_TOKENS_PER_GPU:,}\")\n",
    "print(f\"Max Sequence Length: {MAX_SEQ_LEN:,}\")\n",
    "print()\n",
    "print(f\"Save Final Checkpoint: {SAVE_FINAL_CHECKPOINT}\")\n",
    "print(f\"Checkpoint at Epoch: {CHECKPOINT_AT_EPOCH}\")\n",
    "print()\n",
    "print(f\"Distributed: {NUM_GPUS} GPUs √ó {NUM_NODES} nodes = {NUM_GPUS * NUM_NODES} total GPUs\")\n",
    "print(f\"Node Rank: {NODE_RANK}\")\n",
    "print(f\"RDZV ID: {RDZV_ID}\")\n",
    "print(f\"RDZV Endpoint: {RDZV_ENDPOINT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4b74a2",
   "metadata": {},
   "source": [
    "## Clean up GPU Memory\n",
    "\n",
    "Make sure to clean up all excess memory used earlier so we can train without OOM errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44ecd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "gc.collect()\n",
    "print(\"üíæ Clearing CUDA cache...\")\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "torch.cuda.synchronize()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee601ca3",
   "metadata": {},
   "source": [
    "## Fine-tuning the model using OSFT\n",
    "\n",
    "Since prompting the model doesn't work, our next step is modifying the model.\n",
    "Normally, this wouldn't be a great solution since many methods can cause the model to forget important capabilities. \n",
    "However; OSFT allows us to adjust the non-critical pieces of the model while keeping the crucial parts intact -- perfect for our use-case üòÉ.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbacfea6",
   "metadata": {},
   "source": [
    "## Preparing to train\n",
    "\n",
    "We have to unset the memory settings we enabled earlier so that we do not run into issues when training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df4988d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Set these env variables so we can properly clear the memory after training\n",
    "import os\n",
    "# Unset the environment variables\n",
    "if \"CUDA_LAUNCH_BLOCKING\" in os.environ:\n",
    "    del os.environ[\"CUDA_LAUNCH_BLOCKING\"]\n",
    "\n",
    "if \"PYTORCH_CUDA_ALLOC_CONF\" in os.environ:\n",
    "    del os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8253b12",
   "metadata": {},
   "source": [
    "## Training with OSFT\n",
    "\n",
    "With our hyperparameters configured, now we launch a training job and sit back while it enhances our new model üòéüçø"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94867ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ Starting OSFT Continual Learning Training\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Starting from: {BASE_MODEL_NAME}\")\n",
    "print(f\"Training data: {DATASET_PATH}\")\n",
    "print(f\"Output directory: {CHECKPOINTS_PATH}\")\n",
    "print(f\"Unfreeze ratio: {UNFREEZE_RANK_RATIO}\")\n",
    "print()\n",
    "\n",
    "# Capture output to prevent notebook crashes\n",
    "output_buffer = StringIO()\n",
    "error_buffer = StringIO()\n",
    "\n",
    "training_start_time = time.time()\n",
    "\n",
    "try:\n",
    "    with redirect_stdout(output_buffer), redirect_stderr(error_buffer):\n",
    "        # OSFT training\n",
    "        training_result = osft(\n",
    "            # Model and data\n",
    "            model_path=str(BASE_MODEL_PATH),\n",
    "            data_path=str(DATASET_PATH),\n",
    "            ckpt_output_dir=str(CHECKPOINTS_PATH),\n",
    "            \n",
    "            # OSFT-specific\n",
    "            unfreeze_rank_ratio=UNFREEZE_RANK_RATIO,\n",
    "            \n",
    "            # Training parameters\n",
    "            num_epochs=NUM_EPOCHS,\n",
    "            effective_batch_size=BATCH_SIZE,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            max_seq_len=MAX_SEQ_LEN,\n",
    "            max_tokens_per_gpu=MAX_TOKENS_PER_GPU,\n",
    "            \n",
    "            # Data processing\n",
    "            data_output_dir=str(DATA_OUTPUT_PATH),\n",
    "            warmup_steps=WARMUP_STEPS,\n",
    "            \n",
    "            # Optimization\n",
    "            use_liger=USE_LIGER,\n",
    "            seed=SEED,\n",
    "            lr_scheduler=LR_SCHEDULER,\n",
    "            \n",
    "            # Checkpointing\n",
    "            checkpoint_at_epoch=CHECKPOINT_AT_EPOCH,\n",
    "            save_final_checkpoint=SAVE_FINAL_CHECKPOINT,\n",
    "            \n",
    "            # Distributed training\n",
    "            nproc_per_node=NUM_GPUS,\n",
    "            nnodes=NUM_NODES,\n",
    "            node_rank=NODE_RANK,\n",
    "            rdzv_id=RDZV_ID,\n",
    "            rdzv_endpoint=RDZV_ENDPOINT,\n",
    "        )\n",
    "    \n",
    "    training_duration = time.time() - training_start_time\n",
    "\n",
    "    # Find the most recent checkpoint from Phase10 training\n",
    "    final_checkpoint = find_most_recent_checkpoint(CHECKPOINTS_PATH)\n",
    "    print(\"\\n\\n\\n‚úÖ Model Training Completed\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"üìÅ Final model checkpoint: {final_checkpoint}\")\n",
    "\n",
    "    fine_tuned_model_path = save_best_model(final_checkpoint)\n",
    "    \n",
    "    print(f\"‚úÖ OSFT training completed  in {training_duration/3600:.2f} hours!\")\n",
    "    print()\n",
    "    print(\"üìä Training Achievements:\")\n",
    "    print(\"  ‚Ä¢ Base model capabilities: ‚úÖ Preserved\")\n",
    "    print(\"  ‚Ä¢ New knowledge integrated: ‚úÖ Complete\")\n",
    "    print(\"  ‚Ä¢ Continual learning: ‚úÖ Success\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå OSFT training failed: {e}\")\n",
    "    print(\"\\nError details:\")\n",
    "    print(error_buffer.getvalue())\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17782977",
   "metadata": {},
   "source": [
    "## Best Practices: Choosing Your Unfreeze Rank Ratio\n",
    "\n",
    "The `unfreeze_rank_ratio` is your key control for balancing learning vs. preservation. Here's how to choose the right value for your use case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec1a053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PRACTICAL GUIDE: UNFREEZE RANK RATIO SELECTION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üìö Choosing the Right Unfreeze Rank Ratio\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Scenario 1: Small behavior tweaks\n",
    "print(\"1Ô∏è‚É£  **Small Behavior Tweaks**\")\n",
    "print(\"   When to use: Adjusting specific model behaviors without major changes\")\n",
    "print(\"   Examples: Output formatting, response style, minor corrections\")\n",
    "print()\n",
    "print(\"   üéØ Strategy: Start SMALL\")\n",
    "print(\"   ‚Ä¢ unfreeze_rank_ratio = 0.1 - 0.15\")\n",
    "print(\"   ‚Ä¢ Why: Minimal modification preserves most model behavior\")\n",
    "print(\"   ‚Ä¢ Result: Targeted changes without broad impact\")\n",
    "print()\n",
    "\n",
    "# Scenario 2: Major new capabilities\n",
    "print(\"2Ô∏è‚É£  **Major New Capabilities**\")\n",
    "print(\"   When to use: Adding entirely new skills or knowledge domains\")\n",
    "print(\"   Examples: New language, coding ability, domain expertise\")\n",
    "print()\n",
    "print(\"   üéØ Strategy: Start STANDARD\")\n",
    "print(\"   ‚Ä¢ unfreeze_rank_ratio = 0.3 - 0.35\")\n",
    "print(\"   ‚Ä¢ Why: More freedom to learn complex new patterns\")\n",
    "print(\"   ‚Ä¢ Result: Robust new capabilities while preserving base model\")\n",
    "print()\n",
    "\n",
    "# Scenario 3: Sequential task learning\n",
    "print(\"3Ô∏è‚É£  **Sequential Task Learning (Task 1 ‚Üí Task 2)**\")\n",
    "print(\"   When to use: Training on multiple tasks in sequence\")\n",
    "print(\"   Examples: Knowledge ‚Üí Skills, General ‚Üí Specialized\")\n",
    "print()\n",
    "print(\"   üéØ Strategy: PROGRESSIVELY REDUCE\")\n",
    "print(\"   ‚Ä¢ Task 1: unfreeze_rank_ratio = 0.35\")\n",
    "print(\"   ‚Ä¢ Task 2: unfreeze_rank_ratio = 0.30 (reduce by 0.05)\")\n",
    "print(\"   ‚Ä¢ Task 3: unfreeze_rank_ratio = 0.25 (reduce by 0.05)\")\n",
    "print(\"   ‚Ä¢ Why: Each reduction preserves previous learning\")\n",
    "print(\"   ‚Ä¢ Result: Accumulate capabilities without forgetting\")\n",
    "print()\n",
    "\n",
    "# Golden rules\n",
    "print(\"üåü Golden Rules:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"‚Ä¢ Never go below 0.1 (too restrictive for learning)\")\n",
    "print(\"‚Ä¢ Never go above 0.5 (risks forgetting)\")\n",
    "print(\"‚Ä¢ When in doubt, start smaller - you can always increase\")\n",
    "print(\"‚Ä¢ Test preservation after each training phase\")\n",
    "print()\n",
    "\n",
    "# Quick reference table\n",
    "print(\"üìä Quick Reference:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"| Use Case                | Recommended Ratio | Notes                    |\")\n",
    "print(\"|------------------------|-------------------|--------------------------|\")\n",
    "print(\"| Format tweaks          | 0.10 - 0.15       | Minimal changes          |\")\n",
    "print(\"| Style adjustments      | 0.15 - 0.20       | Moderate refinement      |\")\n",
    "print(\"| New domain knowledge   | 0.25 - 0.35       | Major capability         |\")\n",
    "print(\"| New task type          | 0.30 - 0.35       | Significant learning     |\")\n",
    "print(\"| Sequential phase 2+    | Previous - 0.05   | Preserve prior phases    |\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23197f7",
   "metadata": {},
   "source": [
    "## Next Steps: Apply OSFT to Your Use Case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182d2db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# NEXT STEPS AND PRACTICAL APPLICATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üöÄ Ready to Use OSFT for Your Own Tasks!\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Code template for your use case\n",
    "print(\"üíª Quick Start Template:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"```python\")\n",
    "print(\"from training_hub import osft\")\n",
    "print(\"\")\n",
    "print(\"# Your OSFT training configuration\")\n",
    "print(\"result = osft(\")\n",
    "print(\"    # Model and data\")\n",
    "print(\"    model_path='meta-llama/Meta-Llama-3-8B-Instruct',\")\n",
    "print(\"    data_path='your_task_data.jsonl',\")\n",
    "print(\"    ckpt_output_dir='./checkpoints/your_experiment',\")\n",
    "print(\"    \")\n",
    "print(\"    # Choose based on your use case:\")\n",
    "print(\"    # - Small tweaks: 0.10-0.15\")\n",
    "print(\"    # - Major capability: 0.30-0.35\") \n",
    "print(\"    # - Sequential training: reduce by 0.05 each phase\")\n",
    "print(\"    unfreeze_rank_ratio=0.2,  # Adjust based on guidance above\")\n",
    "print(\"    \")\n",
    "print(\"    # Standard training parameters\")\n",
    "print(\"    num_epochs=1,\")\n",
    "print(\"    effective_batch_size=64,\")\n",
    "print(\"    learning_rate=5e-6,\")\n",
    "print(\"    max_seq_len=8192,\")\n",
    "print(\"    max_tokens_per_gpu=10000,\")\n",
    "print(\")\")\n",
    "print(\"```\")\n",
    "print()\n",
    "\n",
    "# Testing preservation\n",
    "print(\"üß™ Testing Model Preservation:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"After training, always test that original capabilities remain:\")\n",
    "print()\n",
    "print(\"1. Test on original model's strong areas (general knowledge, reasoning)\")\n",
    "print(\"2. Test on your newly trained capability\")\n",
    "print(\"3. Compare outputs to ensure both work well\")\n",
    "print()\n",
    "\n",
    "# Common use cases\n",
    "print(\"üí° Common OSFT Use Cases:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"‚Ä¢ Adding structured output formats (JSON, XML, tables)\")\n",
    "print(\"‚Ä¢ Teaching domain-specific knowledge without losing general ability\")\n",
    "print(\"‚Ä¢ Adding new language support while preserving others\")\n",
    "print(\"‚Ä¢ Sequential skill building (basic ‚Üí intermediate ‚Üí advanced)\")\n",
    "print(\"‚Ä¢ Customizing response style without breaking functionality\")\n",
    "print()\n",
    "\n",
    "# Final recommendations\n",
    "print(\"üéØ Final Recommendations:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"1. Start with our recommended ratios - they work well\")\n",
    "print(\"2. Use small datasets first to test your approach\")\n",
    "print(\"3. Always evaluate preservation alongside new capabilities\")\n",
    "print(\"4. For production: use the script version for better control\")\n",
    "print(\"5. For sequential training - produce multiple candidate models and advance only the best performing one to the next phase\")\n",
    "print()\n",
    "print(\"Happy training with OSFT - where your model learns without forgetting! üöÄ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a181e93",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Model training has been completed successfully!\n",
    "You can now proceed to the [Evaluation](../05_Evaluation/Evaluation.ipynb) notebook to assess the performance of your fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0572113",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
