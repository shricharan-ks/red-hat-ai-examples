{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b63bb30",
   "metadata": {},
   "source": [
    "# OSFT Continual Learning Demo\n",
    "\n",
    "Fine-tuning language models is hard‚Äîyou need good data, lots of resources, and even small changes can cause problems. This makes it tough to add new abilities to a model. This problem is called **continual learning** and is what our new training technique, orthogonal subspace fine-tuning (OSFT), solves.\n",
    "\n",
    "This notebook presents a hands-on example where we enhance `meta-llama/Meta-Llama-3-8B-Instruct` by teaching it to only produce JSON output when requested.\n",
    "\n",
    "By the end of this notebook, you will learn:\n",
    "- ‚úÖ How Llama can be fine-tuned without destroying its existing capabilities\n",
    "- ‚úÖ How to enhance your own LLMs with OSFT\n",
    "- ‚úÖ Best practices when fine-tuning models\n",
    "- ‚ùå OSFT does NOT kill your existing model when trained on new data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04e296c",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "First, let's import and configure everything that we need to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c38f218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Set these env variables so we can properly clear the memory after training\n",
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "# We have to enable this so we can properly clear the model from memory after inferencing\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:4096,expandable_segments:True\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa4ac16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import training_hub for OSFT training\n",
    "from training_hub import osft\n",
    "\n",
    "# Standard library imports\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import sys\n",
    "from contextlib import redirect_stdout, redirect_stderr\n",
    "from io import StringIO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5842112",
   "metadata": {},
   "source": [
    "## Logging Configuration\n",
    "\n",
    "Set up logging to track progress while preventing notebook crashes from excessive output.\n",
    "<!-- \n",
    "**Note:** For production workflows or long-running jobs, we recommend using the script version at `scripts/osft-training.py` for better logging consistency and resumption capabilities.\n",
    "\n",
    "**Quick script usage:**\n",
    "```bash\n",
    "python scripts/lab_multiphase_osft_training.py \\\n",
    "  --base-model-path /path/to/model \\\n",
    "  --phase07-data-path /path/to/knowledge.jsonl \\\n",
    "  --phase10-data-path /path/to/skills.jsonl \\\n",
    "  --ckpt-output-base-dir /path/to/checkpoints -->\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4350c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging to show only essential information\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Suppress verbose logging from transformers and other libraries\n",
    "logging.getLogger(\"transformers\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"datasets\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"torch\").setLevel(logging.WARNING)\n",
    "\n",
    "print(\"‚úÖ Logging configured for notebook environment\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ae4b87",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "Let's define some helper functions for checkpoint management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1917efda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "\n",
    "def find_most_recent_checkpoint(output_dir):\n",
    "    \"\"\"\n",
    "    Find the most recent checkpoint in the training output directory.\n",
    "    \n",
    "    Args:\n",
    "        output_dir (str): Training output directory containing hf_format/ subdirectory\n",
    "        \n",
    "    Returns:\n",
    "        str: Path to the most recent checkpoint\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If no checkpoints are found\n",
    "    \"\"\"\n",
    "    # Get all checkpoint directories under hf_format\n",
    "    checkpoint_pattern = os.path.join(output_dir, \"hf_format\", \"samples_*.0\")\n",
    "    checkpoint_dirs = glob.glob(checkpoint_pattern)\n",
    "    \n",
    "    if not checkpoint_dirs:\n",
    "        raise ValueError(f\"No checkpoints found in {os.path.join(output_dir, 'hf_format')}\")\n",
    "    \n",
    "    # Find the most recently created checkpoint\n",
    "    most_recent_checkpoint = max(checkpoint_dirs, key=os.path.getctime)\n",
    "    \n",
    "    return most_recent_checkpoint\n",
    "\n",
    "def cleanup_model_memory(*objects):\n",
    "    \"\"\"\n",
    "    Clean up GPU memory by deleting arbitrary objects and clearing CUDA cache.\n",
    "    \n",
    "    Args:\n",
    "        *objects: Variable number of objects to clean up (models, tokenizers, etc.)\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    import gc\n",
    "    \n",
    "    # Delete all provided objects\n",
    "    # Delete objects from global namespace if they exist there\n",
    "    for obj in objects:\n",
    "        if obj is not None:\n",
    "            # Find the variable name in globals that references this object\n",
    "            for var_name, var_obj in list(globals().items()):\n",
    "                if var_obj is obj:\n",
    "                    print(f\"üóëÔ∏è Deleting global variable: {var_name}\")\n",
    "                    del globals()[var_name]\n",
    "                    break\n",
    "            # Also delete the local reference\n",
    "            del obj\n",
    "    \n",
    "    # Force garbage collection\n",
    "    gc.collect()\n",
    "    \n",
    "    # Clear CUDA cache if CUDA is available\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    print(\"‚úÖ Model memory cleaned up and CUDA cache cleared\")\n",
    "\n",
    "\n",
    "print(\"‚úÖ Checkpoint utility functions defined\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8190d331",
   "metadata": {},
   "source": [
    "## LLM Sampling Parameters \n",
    "\n",
    "We will use these below when testing our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6755bcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# üéØ Sampling/Generation Parameters                                            #\n",
    "################################################################################\n",
    "MAX_NEW_TOKENS = 256\n",
    "DO_SAMPLE = True\n",
    "TEMPERATURE = 0.7  # Meta's recommended temperature for Llama\n",
    "TOP_P = 0.9        # Standard top_p for Llama models\n",
    "\n",
    "print(f\"MAX_NEW_TOKENS: {MAX_NEW_TOKENS}\")\n",
    "print(f\"DO_SAMPLE: {DO_SAMPLE}\")\n",
    "print(f\"TEMPERATURE: {TEMPERATURE}\")\n",
    "print(f\"TOP_P: {TOP_P}\")\n",
    "print(\"‚úÖ LLM sampling parameters defined\")\n",
    "print()\n",
    "print(\"üìä Using Meta's recommended Llama sampling settings:\")\n",
    "print(\"  ‚Ä¢ Temperature 0.6 for balanced creativity/consistency\")\n",
    "print(\"  ‚Ä¢ Top-p 0.9 for good token diversity\")\n",
    "print(\"  ‚Ä¢ Stop on both EOS and <|eot_id|> tokens\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a2bf60",
   "metadata": {},
   "source": [
    "## Demonstrating the JSON Output Problem\n",
    "\n",
    "The `meta-llama/Meta-Llama-3-8B-Instruct` model is excellent for general usage, but it struggles with strict JSON output formatting. This is a common issue for agentic applications that need structured responses.\n",
    "\n",
    "Let's test this directly with a table analysis task that requires pure JSON output.\n",
    "\n",
    "For example, let's say we want to ask the model a question about the following table:\n",
    "\n",
    "|Rank|Cyclist|Team|Time|UCI ProTour\\nPoints|\n",
    "|---|---|---|---|---|\n",
    "|1|Alejandro Valverde¬†(ESP)|Caisse d'Epargne|5h 29' 10\"|40|\n",
    "|2|Alexandr Kolobnev¬†(RUS)|Team CSC Saxo Bank|s.t.|30|\n",
    "|3|Davide Rebellin¬†(ITA)|Gerolsteiner|s.t.|25|\n",
    "|4|Paolo Bettini¬†(ITA)|Quick Step|s.t.|20|\n",
    "|5|Franco Pellizotti¬†(ITA)|Liquigas|s.t.|15|\n",
    "|6|Denis Menchov¬†(RUS)|Rabobank|s.t.|11|\n",
    "|7|Samuel S√°nchez¬†(ESP)|Euskaltel-Euskadi|s.t.|7|\n",
    "|8|St√©phane Goubert¬†(FRA)|Ag2r-La Mondiale|+ 2\"|5|\n",
    "|9|Haimar Zubeldia¬†(ESP)|Euskaltel-Euskadi|+ 2\"|3|\n",
    "|10|David Moncouti√©¬†(FRA)|Cofidis|+ 2\"|1|\n",
    "\n",
    "\n",
    "And we want to know: which country had the most players appear in the top 10 list?\n",
    "\n",
    "We can prompt the model like this:\n",
    "```md\n",
    "# Task Description: Please look at the table, and then answer the question. Please also provide an explanation on your answer. Return the final result as JSON in the format {\"answer\": \"<YOUR ANSWER>\"}.\n",
    "\n",
    "## Input:\n",
    "*Table*\n",
    "which country had the most cyclists finish within the top 10?\n",
    "\n",
    "Return the final result as JSON in the format {\"answer\": \"<YOUR ANSWER>\"}.\n",
    "## Output:\n",
    "```\n",
    "\n",
    "Here's the response we get from the vanilla `meta-llama/Meta-LLama-3-8B-Instruct`:\n",
    "\n",
    "\n",
    "```\n",
    "After examining the table, I can answer the question as follows:\n",
    "\n",
    "{\"answer\": \"Spain\"}\n",
    "\n",
    "Explanation:\n",
    "The table shows the top 10 cyclists who finished the race. Upon reviewing the \"Cyclist\" column, I noticed that three cyclists from Spain finished within the top 10: Alejandro Valverde (Rank 1), Samuel S√°nchez (Rank 7), and Haimar Zubeldia (Rank 9). This means that Spain had the most cyclists finish within the top 10.\n",
    "```\n",
    "\n",
    "Although the model provided JSON as part of its output, our instruction explicitly requested the entire answer to be provided as JSON. This response doesn't work, since we would need to write fragile code in order to extract the answer we need -- a pattern we should avoid if possible. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc290b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the vanilla Llama model to demonstrate the JSON output issue\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import json\n",
    "\n",
    "print(\"üöÄ Loading vanilla Llama 3-8B-Instruct model...\")\n",
    "\n",
    "# Load the base model\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "original_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cuda:0\",\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "original_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(f\"‚úÖ Model loaded successfully on GPU 0 with Flash Attention\")\n",
    "print(f\"üìä Model parameters: {original_model.num_parameters():,}\")\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully!\")\n",
    "\n",
    "# Define the test prompt with cycling table data\n",
    "table_data = \"\"\"\n",
    "|Rank|Cyclist|Team|Time|UCI ProTour Points|\n",
    "|---|---|---|---|---|\n",
    "|1|Alejandro Valverde (ESP)|Caisse d'Epargne|5h 29' 10\"|40|\n",
    "|2|Alexandr Kolobnev (RUS)|Team CSC Saxo Bank|s.t.|30|\n",
    "|3|Davide Rebellin (ITA)|Gerolsteiner|s.t.|25|\n",
    "|4|Paolo Bettini (ITA)|Quick Step|s.t.|20|\n",
    "|5|Franco Pellizotti (ITA)|Liquigas|s.t.|15|\n",
    "|6|Denis Menchov (RUS)|Rabobank|s.t.|11|\n",
    "|7|Samuel S√°nchez (ESP)|Euskaltel-Euskadi|s.t.|7|\n",
    "|8|St√©phane Goubert (FRA)|Ag2r-La Mondiale|+ 2\"|5|\n",
    "|9|Haimar Zubeldia (ESP)|Euskaltel-Euskadi|+ 2\"|3|\n",
    "|10|David Moncouti√© (FRA)|Cofidis|+ 2\"|1|\n",
    "\"\"\"\n",
    "\n",
    "user_message = \"\"\"# Task Description: Please look at the table, and then answer the question. Please also provide an explanation on your answer. Return the final result as JSON in the format {\"answer\": \"<YOUR ANSWER>\"}.\n",
    "\n",
    "## Input:\n",
    "*Table*\n",
    "|Rank|Cyclist|Team|Time|UCI ProTour\\nPoints|\n",
    "|---|---|---|---|---|\n",
    "|1|Alejandro Valverde (ESP)|Caisse d'Epargne|5h 29' 10\"|40|\n",
    "|2|Alexandr Kolobnev (RUS)|Team CSC Saxo Bank|s.t.|30|\n",
    "|3|Davide Rebellin (ITA)|Gerolsteiner|s.t.|25|\n",
    "|4|Paolo Bettini (ITA)|Quick Step|s.t.|20|\n",
    "|5|Franco Pellizotti (ITA)|Liquigas|s.t.|15|\n",
    "|6|Denis Menchov (RUS)|Rabobank|s.t.|11|\n",
    "|7|Samuel S√°nchez (ESP)|Euskaltel-Euskadi|s.t.|7|\n",
    "|8|St√©phane Goubert (FRA)|Ag2r-La Mondiale|+ 2\"|5|\n",
    "|9|Haimar Zubeldia (ESP)|Euskaltel-Euskadi|+ 2\"|3|\n",
    "|10|David Moncouti√© (FRA)|Cofidis|+ 2\"|1|\n",
    "*Question:*\n",
    "which country had the most cyclists finish within the top 10?\n",
    "\n",
    "Return the final result as JSON in the format {\"answer\": \"<YOUR ANSWER>\"}.\n",
    "## Output:\"\"\"\n",
    "\n",
    "\n",
    "print(\"üìä Testing model with table reasoning task:\")\n",
    "print(\"Question: Which country had the most cyclists finish within the top 10?\")\n",
    "\n",
    "\n",
    "\n",
    "# Tokenize and generate\n",
    "inputs = original_tokenizer(user_message, return_tensors=\"pt\").to(original_model.device)\n",
    "with torch.no_grad():\n",
    "    outputs = original_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        temperature=TEMPERATURE,  # Meta's recommended temperature\n",
    "        do_sample=DO_SAMPLE,\n",
    "        top_p=TOP_P,\n",
    "    )\n",
    "\n",
    "# Decode the response\n",
    "response = original_tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "print(\"ü§ñ Model response:\")\n",
    "print(\"-\" * 50)\n",
    "print(response)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Try to parse as JSON to demonstrate the issue\n",
    "try:\n",
    "    json_response = json.loads(response.strip())\n",
    "    print(\"‚úÖ Valid JSON response!\")\n",
    "    print(f\"Answer: {json_response}\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(\"‚ùå Invalid JSON response!\")\n",
    "    print(f\"JSON parsing error: {e}\")\n",
    "    print(\"\\nüí° This is the problem OSFT will solve:\")\n",
    "    print(\"   The model gives explanations instead of pure JSON\")\n",
    "    print(\"   Making it unreliable for agentic applications\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4b74a2",
   "metadata": {},
   "source": [
    "## Clean up GPU Memory\n",
    "\n",
    "Once we're done inferencing the model, make sure to clean up all excess memory so we can train without OOM errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44ecd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the CUDA cache\n",
    "del response           # String response\n",
    "del inputs             # üî• GPU tensor\n",
    "del outputs            # üî• GPU tensor  \n",
    "del original_tokenizer # üî• Tokenizer with vocab\n",
    "del original_model      # üî• MOST IMPORTANT - ~15GB model\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "print(\"üíæ Clearing CUDA cache...\")\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "torch.cuda.synchronize()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02006973",
   "metadata": {},
   "source": [
    "## Understanding the Problem\n",
    "\n",
    "As you can see from the output above, the vanilla Llama model struggles with strict JSON formatting requirements. Even when explicitly instructed to respond with \"ONLY valid JSON\", it often:\n",
    "\n",
    "- ‚ùå Adds explanatory text before or after the JSON\n",
    "- ‚ùå Includes reasoning mixed with the JSON output  \n",
    "- ‚ùå Sometimes doesn't follow the exact JSON schema requested\n",
    "\n",
    "This makes it unreliable for agentic applications that need to parse structured responses programmatically.\n",
    "\n",
    "> **Note:** Occasionally the model may respond with the correct format; however, this is a common behavior among LLMs. To be considered capable of JSON output, it must be able to **reliably** and **repeatably** respond with the requested format. \n",
    "\n",
    "## How OSFT Will Solve This\n",
    "\n",
    "We'll use OSFT to teach Llama to produce clean JSON outputs while preserving all its existing capabilities:\n",
    "\n",
    "1. **Preserve existing knowledge**: Llama keeps its reasoning, language understanding, and general capabilities\n",
    "2. **Add new behavior**: Strict adherence to JSON output format when requested\n",
    "3. **No catastrophic forgetting**: The model won't lose its general instruction-following abilities\n",
    "\n",
    "Let's prepare the training data and run OSFT training to fix this issue!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee601ca3",
   "metadata": {},
   "source": [
    "## Fine-tuning the model on TableGPT using OSFT\n",
    "\n",
    "Since prompting the model doesn't work, our next step is modifying the model.\n",
    "Normally, this wouldn't be a great solution since many methods can cause the model to forget important capabilities. \n",
    "However; OSFT allows us to adjust the non-critical pieces of the model while keeping the crucial parts intact -- perfect for our use-case üòÉ.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9c1602",
   "metadata": {},
   "source": [
    "## Downloading the TableGPT dataset\n",
    "\n",
    "To train the model using TableGPT, we need to have the dataset downloaded on our local machine first. We can do this with the `hf` CLI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5ddfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hf download --repo-type dataset 'LipengCS/Table-GPT' --local-dir /path/to/download/location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c791b6c4",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "For this example, I will be running training on an 8xA100 box, but these hyperparameters can be adjusted for any machine; provided it is capable of running OSFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619de35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# ü§ñ Model + Data Paths                                                        #\n",
    "################################################################################\n",
    "BASE_MODEL = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "DATASET_PATH = \"/path/to/downloaded/tablegpt/train/train_All.jsonl\"\n",
    "CHECKPOINTS_PATH = \"/path/to/checkpoints/dir\"\n",
    "DATA_OUTPUT_PATH = \"/dev/shm\"  # for quicker multi-process loading of datasets\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# üèãÔ∏è‚Äç‚ôÄÔ∏è Training Hyperparameters                                                  #\n",
    "################################################################################\n",
    "# Important for OSFT\n",
    "UNFREEZE_RANK_RATIO = 0.25 \n",
    "\n",
    "# Standard parameters\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 5e-6\n",
    "NUM_EPOCHS=2\n",
    "LR_SCHEDULER=\"cosine\"\n",
    "WARMUP_STEPS=0\n",
    "SEED=42\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# üèéÔ∏è Performance Hyperparameters                                               #\n",
    "################################################################################\n",
    "USE_LIGER = True\n",
    "MAX_TOKENS_PER_GPU=10_000\n",
    "MAX_SEQ_LEN=8192\n",
    "\n",
    "################################################################################\n",
    "# üíæ Checkpointing Settings                                                    #\n",
    "################################################################################\n",
    "# Here we only want to save the very last checkpoint\n",
    "SAVE_FINAL_CHECKPOINT = True\n",
    "CHECKPOINT_AT_EPOCH = False \n",
    "\n",
    "################################################################################\n",
    "# üî• TORCHRUN SETTINGS                                                         #\n",
    "################################################################################\n",
    "NUM_GPUS=8\n",
    "NUM_NODES=1\n",
    "NODE_RANK=0\n",
    "RDZV_ID=23\n",
    "RDZV_ENDPOINT='localhost:1738'\n",
    "\n",
    "\n",
    "print(\"‚öôÔ∏è  Training Hyperparameters\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Base Model: {BASE_MODEL}\")\n",
    "print(f\"Dataset Path: {DATASET_PATH}\")\n",
    "print(f\"Checkpoints Path: {CHECKPOINTS_PATH}\")\n",
    "print(f\"Data Output Path: {DATA_OUTPUT_PATH}\")\n",
    "print()\n",
    "print(f\"Unfreeze Rank Ratio: {UNFREEZE_RANK_RATIO}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"Number of Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"LR Scheduler: {LR_SCHEDULER}\")\n",
    "print(f\"Warmup Steps: {WARMUP_STEPS}\")\n",
    "print(f\"Seed: {SEED}\")\n",
    "print()\n",
    "print(f\"Use Liger: {USE_LIGER}\")\n",
    "print(f\"Max Tokens per GPU: {MAX_TOKENS_PER_GPU:,}\")\n",
    "print(f\"Max Sequence Length: {MAX_SEQ_LEN:,}\")\n",
    "print()\n",
    "print(f\"Save Final Checkpoint: {SAVE_FINAL_CHECKPOINT}\")\n",
    "print(f\"Checkpoint at Epoch: {CHECKPOINT_AT_EPOCH}\")\n",
    "print()\n",
    "print(f\"Distributed: {NUM_GPUS} GPUs √ó {NUM_NODES} nodes = {NUM_GPUS * NUM_NODES} total GPUs\")\n",
    "print(f\"Node Rank: {NODE_RANK}\")\n",
    "print(f\"RDZV ID: {RDZV_ID}\")\n",
    "print(f\"RDZV Endpoint: {RDZV_ENDPOINT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbacfea6",
   "metadata": {},
   "source": [
    "## Preparing to train\n",
    "\n",
    "We have to unset the memory settings we enabled earlier so that we do not run into issues when training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df4988d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Set these env variables so we can properly clear the memory after training\n",
    "import os\n",
    "# Unset the environment variables\n",
    "if \"CUDA_LAUNCH_BLOCKING\" in os.environ:\n",
    "    del os.environ[\"CUDA_LAUNCH_BLOCKING\"]\n",
    "\n",
    "if \"PYTORCH_CUDA_ALLOC_CONF\" in os.environ:\n",
    "    del os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8253b12",
   "metadata": {},
   "source": [
    "## Training with OSFT\n",
    "\n",
    "With our hyperparameters configured, now we launch a training job and sit back while it enhances our new model üòéüçø"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94867ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ Starting OSFT Continual Learning Training\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Starting from: {BASE_MODEL}\")\n",
    "print(f\"Training data: {DATASET_PATH}\")\n",
    "print(f\"Output directory: {CHECKPOINTS_PATH}\")\n",
    "print(f\"Unfreeze ratio: {UNFREEZE_RANK_RATIO}\")\n",
    "print()\n",
    "\n",
    "# Capture output to prevent notebook crashes\n",
    "output_buffer = StringIO()\n",
    "error_buffer = StringIO()\n",
    "\n",
    "training_start_time = time.time()\n",
    "\n",
    "try:\n",
    "    with redirect_stdout(output_buffer), redirect_stderr(error_buffer):\n",
    "        # OSFT training\n",
    "        training_result = osft(\n",
    "            # Model and data\n",
    "            model_path=BASE_MODEL,\n",
    "            data_path=DATASET_PATH,\n",
    "            ckpt_output_dir=CHECKPOINTS_PATH,\n",
    "            \n",
    "            # OSFT-specific\n",
    "            unfreeze_rank_ratio=UNFREEZE_RANK_RATIO,\n",
    "            \n",
    "            # Training parameters\n",
    "            num_epochs=NUM_EPOCHS,\n",
    "            effective_batch_size=BATCH_SIZE,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            max_seq_len=MAX_SEQ_LEN,\n",
    "            max_tokens_per_gpu=MAX_TOKENS_PER_GPU,\n",
    "            \n",
    "            # Data processing\n",
    "            data_output_dir=DATA_OUTPUT_PATH,\n",
    "            warmup_steps=WARMUP_STEPS,\n",
    "            \n",
    "            # Optimization\n",
    "            use_liger=USE_LIGER,\n",
    "            seed=SEED,\n",
    "            lr_scheduler=LR_SCHEDULER,\n",
    "            \n",
    "            # Checkpointing\n",
    "            checkpoint_at_epoch=CHECKPOINT_AT_EPOCH,\n",
    "            save_final_checkpoint=SAVE_FINAL_CHECKPOINT,\n",
    "            \n",
    "            # Distributed training\n",
    "            nproc_per_node=NUM_GPUS,\n",
    "            nnodes=NUM_NODES,\n",
    "            node_rank=NODE_RANK,\n",
    "            rdzv_id=RDZV_ID,\n",
    "            rdzv_endpoint=RDZV_ENDPOINT,\n",
    "        )\n",
    "    \n",
    "    training_duration = time.time() - training_start_time\n",
    "\n",
    "    # Find the most recent checkpoint from Phase10 training\n",
    "    final_checkpoint = find_most_recent_checkpoint(CHECKPOINTS_PATH)\n",
    "    print(f\"üìÅ Final model checkpoint: {final_checkpoint}\")\n",
    "\n",
    "    \n",
    "    print(f\"‚úÖ OSFT training completed successfully in {training_duration/3600:.2f} hours!\")\n",
    "    print(f\"üìÅ Checkpoint saved to: {final_checkpoint}\")\n",
    "    print()\n",
    "    print(\"üìä Training Achievements:\")\n",
    "    print(\"  ‚Ä¢ Base model capabilities: ‚úÖ Preserved\")\n",
    "    print(\"  ‚Ä¢ New knowledge integrated: ‚úÖ Complete\")\n",
    "    print(\"  ‚Ä¢ Continual learning: ‚úÖ Success\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå OSFT training failed: {e}\")\n",
    "    print(\"\\nError details:\")\n",
    "    print(error_buffer.getvalue())\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e5edea",
   "metadata": {},
   "source": [
    "## Testing our newly trained model\n",
    "\n",
    "With the model trained, let's load it back in and evaluate it ourselves directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88f5f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load the trained model and tokenizer\n",
    "print(\"üîÑ Loading the trained model...\")\n",
    "trained_tokenizer = AutoTokenizer.from_pretrained(final_checkpoint)\n",
    "trained_model = AutoModelForCausalLM.from_pretrained(\n",
    "    final_checkpoint,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cuda:0\",\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Model loaded successfully on GPU 0 with Flash Attention\")\n",
    "print(f\"üìä Model parameters: {trained_model.num_parameters():,}\")\n",
    "\n",
    "# Test the model with a table reasoning task to evaluate its analytical capabilities\n",
    "user_message = \"\"\"# Task Description: Please look at the table, and then answer the question. Please also provide an explanation on your answer. Return the final result as JSON in the format {\"answer\": \"<YOUR ANSWER>\"}.\n",
    "\n",
    "## Input:\n",
    "*Table*\n",
    "|Rank|Cyclist|Team|Time|UCI ProTour\\nPoints|\n",
    "|---|---|---|---|---|\n",
    "|1|Alejandro Valverde (ESP)|Caisse d'Epargne|5h 29' 10\"|40|\n",
    "|2|Alexandr Kolobnev (RUS)|Team CSC Saxo Bank|s.t.|30|\n",
    "|3|Davide Rebellin (ITA)|Gerolsteiner|s.t.|25|\n",
    "|4|Paolo Bettini (ITA)|Quick Step|s.t.|20|\n",
    "|5|Franco Pellizotti (ITA)|Liquigas|s.t.|15|\n",
    "|6|Denis Menchov (RUS)|Rabobank|s.t.|11|\n",
    "|7|Samuel S√°nchez (ESP)|Euskaltel-Euskadi|s.t.|7|\n",
    "|8|St√©phane Goubert (FRA)|Ag2r-La Mondiale|+ 2\"|5|\n",
    "|9|Haimar Zubeldia (ESP)|Euskaltel-Euskadi|+ 2\"|3|\n",
    "|10|David Moncouti√© (FRA)|Cofidis|+ 2\"|1|\n",
    "*Question:*\n",
    "which country had the most cyclists finish within the top 10?\n",
    "\n",
    "Return the final result as JSON in the format {\"answer\": \"<YOUR ANSWER>\"}.\n",
    "## Output:\"\"\"\n",
    "\n",
    "# Format the message (adjust based on your model's chat template)\n",
    "if hasattr(trained_tokenizer, 'apply_chat_template'):\n",
    "    messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "    formatted_input = trained_tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "else:\n",
    "    # Fallback formatting\n",
    "    formatted_input = f\"User: {user_message}\\nAssistant:\"\n",
    "\n",
    "print(\"üìä Testing model with table reasoning task:\")\n",
    "print(\"Question: Which country had the most cyclists finish within the top 10?\")\n",
    "print(\"ü§ñ Model response:\")\n",
    "\n",
    "# Tokenize and generate\n",
    "inputs = trained_tokenizer(formatted_input, return_tensors=\"pt\").to(trained_model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = trained_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        do_sample=DO_SAMPLE,\n",
    "        temperature=TEMPERATURE,\n",
    "        top_p=TOP_P,\n",
    "    )\n",
    "\n",
    "# Decode and print the response\n",
    "response = trained_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "# Remove the input part to show only the generated response\n",
    "if formatted_input in response:\n",
    "    response = response.replace(formatted_input, \"\").strip()\n",
    "\n",
    "print(response)\n",
    "print(\"\\n‚úÖ Table reasoning test completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c0fcf1",
   "metadata": {},
   "source": [
    "## Testing Knowledge Retention: Science Questions\n",
    "\n",
    "Now let's verify that our OSFT-trained model retains its original knowledge capabilities. Even though we taught it to produce strict JSON output, it should still be able to handle complex scientific reasoning.\n",
    "\n",
    "We'll test the model on a scientific question to ensure **no catastrophic forgetting** occurred during our JSON formatting training.\n",
    "\n",
    "**Question:** Using scientific terminology, explain why the sky is blue.\n",
    "**Ground truth:** \n",
    "The sky appears blue due to **Rayleigh scattering** of sunlight by air molecules (mainly N‚ÇÇ and O‚ÇÇ). For particles much smaller than the wavelength, the scattered intensity scales as\n",
    "\n",
    "$$\n",
    "I_{\\text{scat}}(\\lambda, \\theta) ‚àù \\frac{(1 + \\cos^2(Œ∏))}{Œª^4}\n",
    "$$\n",
    "\n",
    "so **shorter wavelengths** (blue ~450 nm) scatter much more than longer ones (red ~650 nm). Blue light is thus scattered across the sky into your line of sight.\n",
    "\n",
    "**Why not violet?** The Sun emits less violet than blue, human eyes are far less sensitive to violet, and ozone absorbs some violet/UV‚Äîpushing the perceived color toward blue.\n",
    "\n",
    "**Corollaries:** Long slant paths at sunrise/sunset remove blue/green first ‚Üí the Sun looks red/orange; scattered skylight is partially polarized, strongest ~90¬∞ from the Sun.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9249abd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the OSFT-trained model on complex thermodynamics reasoning\n",
    "# This should demonstrate knowledge retention despite JSON training\n",
    "\n",
    "science_question = \"\"\"Using scientific terminology, explain why the sky is blue.\"\"\"\n",
    "\n",
    "print(\"üî¨ Testing Science Knowledge Retention\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Question:\")\n",
    "print(science_question)\n",
    "print(\"\\nü§ñ OSFT-trained model response:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Use the trained model that should still be loaded\n",
    "inputs = trained_tokenizer(science_question, return_tensors=\"pt\").to(trained_model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = trained_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,  # More tokens for complex reasoning\n",
    "        temperature=TEMPERATURE,  # Meta's recommended temperature\n",
    "        do_sample=DO_SAMPLE,\n",
    "        top_p=TOP_P,\n",
    "    )\n",
    "\n",
    "# Decode the response\n",
    "response = trained_tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "print(response)\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(\"\\n‚úÖ Science Knowledge Test Results:\")\n",
    "print(\"üìä The model should demonstrate:\")\n",
    "print(\"  ‚Ä¢ Clear explanations with scientific terminology\")\n",
    "print(\"  ‚Ä¢ Step-by-step mathematical reasoning\")\n",
    "print(\"  ‚Ä¢ Proper use of scientific relationships\")\n",
    "print(\"\\nüéØ Key Insight:\")\n",
    "print(\"  Despite learning strict JSON formatting, the model retains\")\n",
    "print(\"  its complex scientific reasoning capabilities!\")\n",
    "print(\"  This proves OSFT successfully avoided catastrophic forgetting.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55166bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with the original vanilla model on the same thermodynamics question\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "print(\"üî¨ Testing Original Model on Same Science Question\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Reload the original vanilla model for comparison\n",
    "print(\"üì• Reloading vanilla Llama 3-8B-Instruct model with Flash Attention...\")\n",
    "device = \"cuda:0\"\n",
    "original_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": device},  # Place on GPU\n",
    "    attn_implementation=\"flash_attention_2\"  # Use Flash Attention\n",
    ")\n",
    "original_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "\n",
    "print(f\"‚úÖ Original model reloaded successfully on GPU {device} with Flash Attention!\")\n",
    "\n",
    "print(\"\\nUsing the same science question with the vanilla model...\")\n",
    "print(\"\\nü§ñ Original model response:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Use the reloaded original model\n",
    "inputs = original_tokenizer(science_question, return_tensors=\"pt\").to(original_model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = original_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=500,  # More tokens for complex reasoning\n",
    "        temperature=TEMPERATURE,  # Meta's recommended temperature\n",
    "        do_sample=DO_SAMPLE,\n",
    "        top_p=TOP_P,\n",
    "    )\n",
    "\n",
    "# Decode the response\n",
    "original_response = original_tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "print(original_response)\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(\"\\nüìä Comparison Analysis:\")\n",
    "print(\"‚úÖ Both models should demonstrate similar scientific reasoning capabilities\")\n",
    "print(\"‚úÖ Both should provide step-by-step mathematical explanations\")\n",
    "print(\"‚úÖ Both should provide correct answers\")\n",
    "print(\"\\nüéØ Key OSFT Insight:\")\n",
    "print(\"  The OSFT-trained model preserves all the original science knowledge\")\n",
    "print(\"  while gaining the new JSON formatting capability!\")\n",
    "print(\"  This is the power of continual learning without catastrophic forgetting.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41ae365",
   "metadata": {},
   "source": [
    "## Final Analysis and Summary\n",
    "\n",
    "In this notebook, we demonstrated how **Orthogonal Subspace Fine-Tuning (OSFT)** solves the continual learning challenge, allowing you to add new capabilities to LLMs without degrading existing ones. OSFT enables models to learn new tasks while preserving their original knowledge through mathematical orthogonality.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda1f24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# OSFT TRAINING SUMMARY AND KEY INSIGHTS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üéâ OSFT Continual Learning Complete!\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# What we demonstrated\n",
    "print(\"üìä What We Demonstrated:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"‚úÖ Base model: Struggled with pure JSON output\")\n",
    "print(\"‚úÖ OSFT-trained model: Perfect JSON output when requested\") \n",
    "print(\"‚úÖ Knowledge preserved: All original capabilities intact\")\n",
    "print(\"‚úÖ No catastrophic forgetting: Scientific reasoning still works\")\n",
    "print()\n",
    "\n",
    "# Key advantage over traditional fine-tuning\n",
    "print(\"üî• OSFT vs Traditional Fine-Tuning:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Traditional SFT:\")\n",
    "print(\"  ‚Ä¢ Risk of catastrophic forgetting\")\n",
    "print(\"  ‚Ä¢ Need careful data mixing to preserve capabilities\")\n",
    "print(\"  ‚Ä¢ Complex replay buffers for multi-task learning\")\n",
    "print()\n",
    "print(\"OSFT:\")\n",
    "print(\"  ‚Ä¢ Built-in protection against forgetting\")\n",
    "print(\"  ‚Ä¢ Just train on new data - no mixing needed\")\n",
    "print(\"  ‚Ä¢ Orthogonal learning preserves existing knowledge\")\n",
    "print()\n",
    "\n",
    "# The power of orthogonal learning\n",
    "print(\"üí° The Power of Orthogonal Learning:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"OSFT trains in a mathematical subspace that doesn't interfere\")\n",
    "print(\"with existing model knowledge. This means:\")\n",
    "print()\n",
    "print(\"  1. New capabilities don't overwrite old ones\")\n",
    "print(\"  2. Model retains all base abilities\")\n",
    "print(\"  3. Can add multiple capabilities sequentially\")\n",
    "print(\"  4. No need for complex data replay strategies\")\n",
    "print()\n",
    "\n",
    "print(\"‚ú® Result: The best of both worlds - new skills AND preserved knowledge!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17782977",
   "metadata": {},
   "source": [
    "## Best Practices: Choosing Your Unfreeze Rank Ratio\n",
    "\n",
    "The `unfreeze_rank_ratio` is your key control for balancing learning vs. preservation. Here's how to choose the right value for your use case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec1a053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PRACTICAL GUIDE: UNFREEZE RANK RATIO SELECTION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üìö Choosing the Right Unfreeze Rank Ratio\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Scenario 1: Small behavior tweaks\n",
    "print(\"1Ô∏è‚É£  **Small Behavior Tweaks**\")\n",
    "print(\"   When to use: Adjusting specific model behaviors without major changes\")\n",
    "print(\"   Examples: Output formatting, response style, minor corrections\")\n",
    "print()\n",
    "print(\"   üéØ Strategy: Start SMALL\")\n",
    "print(\"   ‚Ä¢ unfreeze_rank_ratio = 0.1 - 0.15\")\n",
    "print(\"   ‚Ä¢ Why: Minimal modification preserves most model behavior\")\n",
    "print(\"   ‚Ä¢ Result: Targeted changes without broad impact\")\n",
    "print()\n",
    "\n",
    "# Scenario 2: Major new capabilities\n",
    "print(\"2Ô∏è‚É£  **Major New Capabilities**\")\n",
    "print(\"   When to use: Adding entirely new skills or knowledge domains\")\n",
    "print(\"   Examples: New language, coding ability, domain expertise\")\n",
    "print()\n",
    "print(\"   üéØ Strategy: Start STANDARD\")\n",
    "print(\"   ‚Ä¢ unfreeze_rank_ratio = 0.3 - 0.35\")\n",
    "print(\"   ‚Ä¢ Why: More freedom to learn complex new patterns\")\n",
    "print(\"   ‚Ä¢ Result: Robust new capabilities while preserving base model\")\n",
    "print()\n",
    "\n",
    "# Scenario 3: Sequential task learning\n",
    "print(\"3Ô∏è‚É£  **Sequential Task Learning (Task 1 ‚Üí Task 2)**\")\n",
    "print(\"   When to use: Training on multiple tasks in sequence\")\n",
    "print(\"   Examples: Knowledge ‚Üí Skills, General ‚Üí Specialized\")\n",
    "print()\n",
    "print(\"   üéØ Strategy: PROGRESSIVELY REDUCE\")\n",
    "print(\"   ‚Ä¢ Task 1: unfreeze_rank_ratio = 0.35\")\n",
    "print(\"   ‚Ä¢ Task 2: unfreeze_rank_ratio = 0.30 (reduce by 0.05)\")\n",
    "print(\"   ‚Ä¢ Task 3: unfreeze_rank_ratio = 0.25 (reduce by 0.05)\")\n",
    "print(\"   ‚Ä¢ Why: Each reduction preserves previous learning\")\n",
    "print(\"   ‚Ä¢ Result: Accumulate capabilities without forgetting\")\n",
    "print()\n",
    "\n",
    "# Golden rules\n",
    "print(\"üåü Golden Rules:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"‚Ä¢ Never go below 0.1 (too restrictive for learning)\")\n",
    "print(\"‚Ä¢ Never go above 0.5 (risks forgetting)\")\n",
    "print(\"‚Ä¢ When in doubt, start smaller - you can always increase\")\n",
    "print(\"‚Ä¢ Test preservation after each training phase\")\n",
    "print()\n",
    "\n",
    "# Quick reference table\n",
    "print(\"üìä Quick Reference:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"| Use Case                | Recommended Ratio | Notes                    |\")\n",
    "print(\"|------------------------|-------------------|--------------------------|\")\n",
    "print(\"| Format tweaks          | 0.10 - 0.15       | Minimal changes          |\")\n",
    "print(\"| Style adjustments      | 0.15 - 0.20       | Moderate refinement      |\")\n",
    "print(\"| New domain knowledge   | 0.25 - 0.35       | Major capability         |\")\n",
    "print(\"| New task type          | 0.30 - 0.35       | Significant learning     |\")\n",
    "print(\"| Sequential phase 2+    | Previous - 0.05   | Preserve prior phases    |\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23197f7",
   "metadata": {},
   "source": [
    "## Next Steps: Apply OSFT to Your Use Case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182d2db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# NEXT STEPS AND PRACTICAL APPLICATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üöÄ Ready to Use OSFT for Your Own Tasks!\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Code template for your use case\n",
    "print(\"üíª Quick Start Template:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"```python\")\n",
    "print(\"from training_hub import osft\")\n",
    "print(\"\")\n",
    "print(\"# Your OSFT training configuration\")\n",
    "print(\"result = osft(\")\n",
    "print(\"    # Model and data\")\n",
    "print(\"    model_path='meta-llama/Meta-Llama-3-8B-Instruct',\")\n",
    "print(\"    data_path='your_task_data.jsonl',\")\n",
    "print(\"    ckpt_output_dir='./checkpoints/your_experiment',\")\n",
    "print(\"    \")\n",
    "print(\"    # Choose based on your use case:\")\n",
    "print(\"    # - Small tweaks: 0.10-0.15\")\n",
    "print(\"    # - Major capability: 0.30-0.35\") \n",
    "print(\"    # - Sequential training: reduce by 0.05 each phase\")\n",
    "print(\"    unfreeze_rank_ratio=0.2,  # Adjust based on guidance above\")\n",
    "print(\"    \")\n",
    "print(\"    # Standard training parameters\")\n",
    "print(\"    num_epochs=1,\")\n",
    "print(\"    effective_batch_size=64,\")\n",
    "print(\"    learning_rate=5e-6,\")\n",
    "print(\"    max_seq_len=8192,\")\n",
    "print(\"    max_tokens_per_gpu=10000,\")\n",
    "print(\")\")\n",
    "print(\"```\")\n",
    "print()\n",
    "\n",
    "# Testing preservation\n",
    "print(\"üß™ Testing Model Preservation:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"After training, always test that original capabilities remain:\")\n",
    "print()\n",
    "print(\"1. Test on original model's strong areas (general knowledge, reasoning)\")\n",
    "print(\"2. Test on your newly trained capability\")\n",
    "print(\"3. Compare outputs to ensure both work well\")\n",
    "print()\n",
    "\n",
    "# Common use cases\n",
    "print(\"üí° Common OSFT Use Cases:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"‚Ä¢ Adding structured output formats (JSON, XML, tables)\")\n",
    "print(\"‚Ä¢ Teaching domain-specific knowledge without losing general ability\")\n",
    "print(\"‚Ä¢ Adding new language support while preserving others\")\n",
    "print(\"‚Ä¢ Sequential skill building (basic ‚Üí intermediate ‚Üí advanced)\")\n",
    "print(\"‚Ä¢ Customizing response style without breaking functionality\")\n",
    "print()\n",
    "\n",
    "# Final recommendations\n",
    "print(\"üéØ Final Recommendations:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"1. Start with our recommended ratios - they work well\")\n",
    "print(\"2. Use small datasets first to test your approach\")\n",
    "print(\"3. Always evaluate preservation alongside new capabilities\")\n",
    "print(\"4. For production: use the script version for better control\")\n",
    "print(\"5. For sequential training - produce multiple candidate models and advance only the best performing one to the next phase\")\n",
    "print()\n",
    "print(\"Happy training with OSFT - where your model learns without forgetting! üöÄ\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
