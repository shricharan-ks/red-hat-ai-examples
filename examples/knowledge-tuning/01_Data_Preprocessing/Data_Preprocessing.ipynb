{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4535f5ad",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "This notebook covers the required preprocessing steps for preparing the `seed.jsonl` dataset which is ready for Synthetic Data Generation (SDG). \n",
    "\n",
    "1. Configure the paths\n",
    "2. Loading the files into memory\n",
    "3. Document Conversion\n",
    "4. Chunking\n",
    "5. Saving the processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b27fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "from docling.chunking import HybridChunker\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "\n",
    "from ai_tools.usecase.knowledge_tuning.create_seed_dataset import \\\n",
    "    get_seed_dataset\n",
    "from ai_tools.usecase.knowledge_tuning.utils import (generate_seed_examples,\n",
    "                                                     review_seed_examples_file,\n",
    "                                                     view_seed_example)\n",
    "\n",
    "WORKSPACE = Path.cwd().parent  # Path to the workspace directory\n",
    "\n",
    "SOURCE_DOCUMENT_DIR = WORKSPACE / \"source_documents\"\n",
    "OUTPUT_DIR = WORKSPACE / \"output\" / \"step_01\"\n",
    "\n",
    "OUTPUT_DIR.mkdir(\n",
    "    parents=True, exist_ok=True\n",
    ")  # Create output directory if it doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e411e44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_files = SOURCE_DOCUMENT_DIR.glob(\"**/*.pdf\")\n",
    "available_files = list(available_files)\n",
    "\n",
    "print(f\"Total PDF files found: {len(available_files)} \\n\")\n",
    "print(\"Available Files:\")\n",
    "\n",
    "for file in available_files:\n",
    "    print(f\"\\t{file.stem}.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975c02fd",
   "metadata": {},
   "source": [
    "## Document Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82694f1f",
   "metadata": {},
   "source": [
    "The source documents are in pdf format and we will be using `docling` to read and convert them into docling output format.\n",
    "\n",
    "configuring docling pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ee7c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_options = PdfPipelineOptions()\n",
    "doc_converter = DocumentConverter(\n",
    "    format_options={InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a33fb2",
   "metadata": {},
   "source": [
    "Convert the document to docling format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960baf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_report = {}\n",
    "for file in available_files:\n",
    "    conv_result = doc_converter.convert(file)\n",
    "\n",
    "    document = conv_result.document\n",
    "    confidence_report[file.stem] = conv_result.confidence\n",
    "\n",
    "    document_dict = document.export_to_dict()\n",
    "\n",
    "    (OUTPUT_DIR / \"docling_output\").mkdir(parents=True, exist_ok=True)\n",
    "    output_file = OUTPUT_DIR / \"docling_output\" / f\"{file.stem}.json\"\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(document_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318f18a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file, confidence_report in confidence_report.items():\n",
    "    print(f\"Conversion confidence for {file}:\")\n",
    "\n",
    "    print(\n",
    "        f\"Average confidence: \\x1b[1m{confidence_report.mean_grade.name}\\033[0m (score {confidence_report.mean_score:.3f})\"\n",
    "    )\n",
    "\n",
    "    low_score_pages = []\n",
    "    for page in confidence_report.pages:\n",
    "        page_confidence_report = confidence_report.pages[page]\n",
    "        if page_confidence_report.mean_score < confidence_report.mean_score:\n",
    "            low_score_pages.append(page)\n",
    "\n",
    "    print(\n",
    "        f\"Pages that scored lower than average: {', '.join(str(x + 1) for x in low_score_pages)}\"\n",
    "    )\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3178dc",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008c0ac9",
   "metadata": {},
   "source": [
    "Chunk the document using docling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1add96",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker = HybridChunker()\n",
    "\n",
    "all_chunks = []\n",
    "\n",
    "json_files = (OUTPUT_DIR / \"docling_output\").glob(\"*.json\")\n",
    "\n",
    "convertor = DocumentConverter()\n",
    "\n",
    "for file in json_files:\n",
    "    conv_result = convertor.convert(file)\n",
    "\n",
    "    chunks = chunker.chunk(conv_result.document)\n",
    "    chunks = list(chunks)\n",
    "    print(f\"Total chunks created for {file.stem}: {len(chunks)}\")\n",
    "\n",
    "    for chunk in chunks:\n",
    "        all_chunks.append(\n",
    "            {\n",
    "                \"chunk\": chunker.contextualize(chunk),\n",
    "                \"file\": file.stem,\n",
    "                \"metadata\": chunk.meta.export_json_dict(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "chunks_file_path = OUTPUT_DIR / \"chunks.jsonl\"\n",
    "with open(chunks_file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    for chunk in all_chunks:\n",
    "        json.dump(chunk, file)\n",
    "        file.write(\"\\n\")\n",
    "    print(f\"Path of chunks JSON is: {Path(chunks_file_path).resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295bfb8d",
   "metadata": {},
   "source": [
    "View random chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c4510c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CHUNKS_TO_VIEW = 5\n",
    "\n",
    "\n",
    "sample = random.sample(all_chunks, min(len(all_chunks), NUM_CHUNKS_TO_VIEW))\n",
    "\n",
    "i = 1\n",
    "for chunk in sample:\n",
    "    print(f\"== Randomly selected chunk {i}: ==========\\n\\n{chunk['chunk']}\\n\\n\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3179f5f6",
   "metadata": {},
   "source": [
    "Read the chunks back from the saved file and then randomly select the chunks to use as seed data for SDG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e275e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = []\n",
    "with open(chunks_file_path, encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        chunk = json.loads(line)\n",
    "        chunks.append(chunk)\n",
    "\n",
    "NUM_SEED_EXAMPLES = 5  # Number of chunks to select as seed examples\n",
    "selected_chunks = random.sample(chunks, NUM_SEED_EXAMPLES)\n",
    "selected_chunks_path = OUTPUT_DIR / \"selected_chunks.jsonl\"\n",
    "with open(selected_chunks_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    for chunk in selected_chunks:\n",
    "        json.dump(chunk, file)\n",
    "        file.write(\"\\n\")\n",
    "    print(f\"Path of selected chunks JSON is: {Path(selected_chunks_path).resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee6682e",
   "metadata": {},
   "source": [
    "## QnA.yaml file Generation\n",
    "\n",
    "Generate QnA for each chunk selected above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1899f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your API credentials and model details\n",
    "API_KEY = \"\"  # Replace with your actual API key\n",
    "ENDPOINT = \"https://granite-3-3-8b-instruct-maas-apicast-production.apps.prod.rhoai.rh-aiservices-bu.com:443/v1\"\n",
    "MODEL_NAME = \"granite-3-3-8b-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f617e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOMISATION_PROMPT = \"Generate atleast 5 seed examples in the format specified below.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd160498",
   "metadata": {},
   "source": [
    "For every chunk in the randomly selected chunks, we will create a QnA pair in the `QnA.yaml` file. \n",
    "\n",
    "The generation will be done using a LLM, with a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0ffc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_seed_examples(\n",
    "    \"\",\n",
    "    selected_chunks_path,\n",
    "    OUTPUT_DIR,\n",
    "    API_KEY,\n",
    "    ENDPOINT,\n",
    "    MODEL_NAME,\n",
    "    \"DOMAIN\",\n",
    "    \"SUMMARY\",\n",
    "    CUSTOMISATION_PROMPT,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa13b5cd",
   "metadata": {},
   "source": [
    "View the a seed example from the QnA.yaml file generated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488bf167",
   "metadata": {},
   "outputs": [],
   "source": [
    "view_seed_example(OUTPUT_DIR / \"qna.yaml\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5efbea8",
   "metadata": {},
   "source": [
    "Review the generated QnA pairs.\n",
    "\n",
    "- Checks to find the presence of required fields\n",
    "- Check the number of seed examples generated\n",
    "- Check the number of QnA pairs generated for each seed example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6788f2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_seed_examples_file(OUTPUT_DIR / \"qna.yaml\", min_seed_examples=4, num_qa_pairs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab1405d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_data = get_seed_dataset(OUTPUT_DIR, OUTPUT_DIR)\n",
    "seed_data\n",
    "seed_data.to_json(OUTPUT_DIR / \"final_seed_data.jsonl\", orient=\"records\", lines=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "01_Data_Preprocessing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
