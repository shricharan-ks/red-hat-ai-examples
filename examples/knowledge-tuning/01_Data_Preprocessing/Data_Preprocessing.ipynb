{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4535f5ad",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "This notebook covers the required preprocessing steps for preparing the `seed.jsonl` dataset which is ready for Synthetic Data Generation (SDG). \n",
    "\n",
    "1. Configure the paths\n",
    "2. Loading the files into memory\n",
    "3. Document Conversion\n",
    "4. Chunking\n",
    "5. Saving the processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b27fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "WORKSPACE = Path.cwd().parent # Path to the workspace directory\n",
    "\n",
    "SOURCE_DOCUMENT_DIR= WORKSPACE / \"source_documents\"\n",
    "OUTPUT_DIR= WORKSPACE / \"output\" / \"step_01\"\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)  # Create output directory if it doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e411e44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_files = SOURCE_DOCUMENT_DIR.glob(\"**/*.pdf\")\n",
    "available_files = list(available_files)\n",
    "\n",
    "print(f\"Total PDF files found: {len(available_files)} \\n\")\n",
    "print(\"Available Files:\")\n",
    "\n",
    "for file in available_files:\n",
    "    print(f\"\\t{file.stem}.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975c02fd",
   "metadata": {},
   "source": [
    "## Document Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82694f1f",
   "metadata": {},
   "source": [
    "The source documents are in pdf format and we will be using `docling` to read and convert them into docling output format.\n",
    "\n",
    "configuring docling pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ee7c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.datamodel.accelerator_options import AcceleratorOptions, AcceleratorDevice\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import EasyOcrOptions, PdfPipelineOptions, VlmPipelineOptions, smoldocling_vlm_conversion_options\n",
    "\n",
    "from docling.pipeline.vlm_pipeline import VlmPipeline\n",
    "from docling.backend.docling_parse_v4_backend import DoclingParseV4DocumentBackend\n",
    "\n",
    "pipeline_options = PdfPipelineOptions()\n",
    "doc_converter = DocumentConverter(\n",
    "    format_options={\n",
    "        InputFormat.PDF: PdfFormatOption(\n",
    "            pipeline_options=pipeline_options\n",
    "        )\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a33fb2",
   "metadata": {},
   "source": [
    "Convert the document to docling format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960baf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_report = {}\n",
    "for file in available_files:\n",
    "\n",
    "    conv_result = doc_converter.convert(file)\n",
    "\n",
    "    document = conv_result.document\n",
    "    confidence_report[file.stem] = conv_result.confidence\n",
    "\n",
    "    document_dict = document.export_to_dict()\n",
    "\n",
    "    (OUTPUT_DIR / \"docling_output\").mkdir(parents=True, exist_ok=True)\n",
    "    output_file = OUTPUT_DIR / \"docling_output\" /  f\"{file.stem}.json\"\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(document_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318f18a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file, confidence_report in confidence_report.items():\n",
    "    print(f\"Conversion confidence for {file}:\")\n",
    "    \n",
    "    print(f\"Average confidence: \\x1b[1m{confidence_report.mean_grade.name}\\033[0m (score {confidence_report.mean_score:.3f})\")\n",
    "    \n",
    "    low_score_pages = []\n",
    "    for page in confidence_report.pages:\n",
    "        page_confidence_report = confidence_report.pages[page]\n",
    "        if page_confidence_report.mean_score < confidence_report.mean_score:\n",
    "            low_score_pages.append(page)\n",
    "\n",
    "    print(f\"Pages that scored lower than average: {', '.join(str(x + 1) for x in low_score_pages)}\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3178dc",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008c0ac9",
   "metadata": {},
   "source": [
    "Chunk the document using docling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1add96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.chunking import HybridChunker\n",
    "chunker = HybridChunker()\n",
    "\n",
    "all_chunks = []\n",
    "\n",
    "json_files = (OUTPUT_DIR / \"docling_output\").glob(\"*.json\")\n",
    "\n",
    "convertor = DocumentConverter()\n",
    "\n",
    "for file in json_files:\n",
    "    conv_result = convertor.convert(file)\n",
    "\n",
    "    chunks = chunker.chunk(conv_result.document)\n",
    "    chunks = list(chunks)\n",
    "    print(f\"Total chunks created for {file.stem}: {len(chunks)}\")\n",
    "\n",
    "    for chunk in chunks:\n",
    "        all_chunks.append({\n",
    "            \"chunk\":chunker.contextualize(chunk),\n",
    "            \"file\": file.stem,\n",
    "            \"metadata\":chunk.meta.export_json_dict()\n",
    "        })\n",
    "\n",
    "chunks_file_path = OUTPUT_DIR / \"chunks.jsonl\"\n",
    "with open(chunks_file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    for chunk in all_chunks:\n",
    "        json.dump(chunk, file)\n",
    "        file.write(\"\\n\")\n",
    "    print(f\"Path of chunks JSON is: {Path(chunks_file_path).resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295bfb8d",
   "metadata": {},
   "source": [
    "View random chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c4510c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CHUNKS_TO_VIEW = 5\n",
    "\n",
    "import random\n",
    "import json\n",
    "\n",
    "sample = random.sample(all_chunks, min(len(all_chunks), NUM_CHUNKS_TO_VIEW))\n",
    "\n",
    "i = 1\n",
    "for chunk in sample:\n",
    "    print(f\"== Randomly selected chunk {i}: ==========\\n\\n{chunk['chunk']}\\n\\n\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3179f5f6",
   "metadata": {},
   "source": [
    "Read the chunks back from the saved file and then randomly select the chunks to use as seed data for SDG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e275e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = []\n",
    "with open(chunks_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        chunk = json.loads(line)\n",
    "        chunks.append(chunk)\n",
    "\n",
    "NUM_SEED_EXAMPLES = 5 # Number of chunks to select as seed examples\n",
    "selected_chunks =random.sample(chunks,NUM_SEED_EXAMPLES)\n",
    "selected_chunks_path = OUTPUT_DIR / \"selected_chunks.jsonl\"\n",
    "with open(selected_chunks_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    for chunk in selected_chunks:\n",
    "        json.dump(chunk, file)\n",
    "        file.write(\"\\n\")\n",
    "    print(f\"Path of selected chunks JSON is: {Path(selected_chunks_path).resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee6682e",
   "metadata": {},
   "source": [
    "## QnA.yaml file Generation\n",
    "\n",
    "Generate QnA for each chunk selected above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1899f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your API credentials and model details\n",
    "API_KEY = \"\" # Replace with your actual API key\n",
    "ENDPOINT = \"https://granite-3-3-8b-instruct-maas-apicast-production.apps.prod.rhoai.rh-aiservices-bu.com:443/v1\"\n",
    "MODEL_NAME = \"granite-3-3-8b-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f617e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOMISATION_PROMPT = \"Generate atleast 5 seed examples in the format specified below.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd160498",
   "metadata": {},
   "source": [
    "For every chunk in the randomly selected chunks, we will create a QnA pair in the `QnA.yaml` file. \n",
    "\n",
    "The generation will be done using a LLM, with a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0ffc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai_tools.usecase.knowledge_tuning.utils import generate_seed_examples\n",
    "\n",
    "generate_seed_examples(\n",
    "   \"\",\n",
    "                           selected_chunks_path,\n",
    "                           OUTPUT_DIR,\n",
    "                           API_KEY,\n",
    "                           ENDPOINT,\n",
    "                           MODEL_NAME,\n",
    "                           \"DOMAIN\",\n",
    "                           \"SUMMARY\",                  \n",
    "                           CUSTOMISATION_PROMPT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa13b5cd",
   "metadata": {},
   "source": [
    "View the a seed example from the QnA.yaml file generated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488bf167",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai_tools.usecase.knowledge_tuning.utils import view_seed_example\n",
    "\n",
    "view_seed_example(OUTPUT_DIR / \"qna.yaml\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5efbea8",
   "metadata": {},
   "source": [
    "Review the generated QnA pairs.\n",
    "\n",
    "- Checks to find the presence of required fields\n",
    "- Check the number of seed examples generated\n",
    "- Check the number of QnA pairs generated for each seed example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6788f2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai_tools.usecase.knowledge_tuning.utils import review_seed_examples_file\n",
    "\n",
    "review_seed_examples_file(OUTPUT_DIR / \"qna.yaml\",min_seed_examples=4,num_qa_pairs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab1405d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai_tools.usecase.knowledge_tuning.create_seed_dataset import get_seed_dataset, safe_concatenate_datasets\n",
    "\n",
    "seed_data = get_seed_dataset(OUTPUT_DIR,OUTPUT_DIR)\n",
    "seed_data\n",
    "seed_data.to_json(OUTPUT_DIR/ \"final_seed_data.jsonl\",orient='records', lines=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "01_Data_Preprocessing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
