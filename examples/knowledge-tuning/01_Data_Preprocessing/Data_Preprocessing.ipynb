{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83f458de",
   "metadata": {},
   "source": [
    "# Seed Data Creation\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook takes chunks of a source document and combines them with In Context Learning (ICL) fields to create a seed_data.jsonl file for the [knowledge generation notebook](https://github.com/Red-Hat-AI-Innovation-Team/sdg_hub/blob/knoweldge_e2e/examples/knowledge_tuning/enhanced_summary_knowledge_tuning/knowledge_generation.ipynb).\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Markdown (.md) file(s) of the source document.\n",
    "- A snippet of the source document that is around 500 tokens in size. This will get used as the `icl_document` below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4427092a-e01d-4e2f-b99d-8bfe9c318a05",
   "metadata": {},
   "source": [
    "## Install Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295749b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq datasets tiktoken docling markdown-it-py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc6af06",
   "metadata": {},
   "source": [
    "## Setup Paths and Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f755561",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "WORKSPACE = Path.cwd().parent  # Path to the workspace directory\n",
    "\n",
    "OUTPUT_DIR = WORKSPACE / \"output\" / \"step_01\"\n",
    "\n",
    "OUTPUT_DIR.mkdir(\n",
    "    parents=True, exist_ok=True\n",
    ")  # Create output directory if it doesn't exist\n",
    "\n",
    "\n",
    "DOCLING_OUTPUT_DIR = OUTPUT_DIR / \"docling_output\"\n",
    "DOCLING_OUTPUT_DIR.mkdir(\n",
    "    parents=True, exist_ok=True\n",
    ") # Create docling output directory if it doesn't exist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b74f433",
   "metadata": {},
   "source": [
    "## Generate Docling Document\n",
    "\n",
    "Convert the source document (PDF, DOCX,HTML, etc.) into markdown format using Docling\n",
    "\n",
    "\n",
    "This example works through the conversion of BMO Website from a URL to markdown using Docling.\n",
    "You can find more documentation on supported file types and usage [here](https://docling.readthedocs.io/en/latest/).\n",
    "\n",
    "\n",
    "#### Data Description\n",
    "- Source Document: [BMO Webpage](https://fintrac-canafe.canada.ca/guidance-directives/client-clientele/Guide11/11-eng)\n",
    "    - ðŸš¨ [Terms and Conditions](https://www.canada.ca/en/transparency/terms.html)\n",
    "\n",
    "\n",
    "NOTE: If you already have the docling markdown file(s) of the source document, you can skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31b7902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "WEB_URLS = [\n",
    "    (\"BMO_data\",\"https://fintrac-canafe.canada.ca/guidance-directives/client-clientele/Guide11/11-eng\")\n",
    "]\n",
    "\n",
    "converter = DocumentConverter()\n",
    "\n",
    "for name,url in WEB_URLS:\n",
    "    result = converter.convert(url)\n",
    "    result.document.save_as_markdown(f\"{DOCLING_OUTPUT_DIR}/{name}.md\")\n",
    "\n",
    "\n",
    "print(f\"Number of md files in {DOCLING_OUTPUT_DIR}: \", len(glob.glob(f'{DOCLING_OUTPUT_DIR}/*.md')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9892143-eddf-4612-aa70-eead88140f86",
   "metadata": {},
   "source": [
    "## Load Converted Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93c0562-ace2-44e7-9884-de95de03ee32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're coming with a docling JSON instead of markdown the following lines will help you convert docling JSON -> .md\n",
    "#converter = DocumentConverter()\n",
    "#result = converter.convert(\"document_collection/ibm-annual-report/ibm-annual-report-2024.json\")\n",
    "#result.document.save_as_markdown(\"document_collection/ibm-annual-report/ibm-annual-report-2024.md\")\n",
    "#print(\"Markown saved to document_collection/ibm-annual-report/ibm-annual-report-2024.md\")\n",
    "\n",
    "\n",
    "# In our example above docling step produces markdown of all the pdf files in the document_collection\n",
    "with open(glob.glob(f'{DOCLING_OUTPUT_DIR}/*.md')[0], 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f0068f",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a9c8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from markdown_it import MarkdownIt  \n",
    "from typing import List\n",
    "import datasets \n",
    "import json\n",
    "\n",
    "\n",
    "def chunk_markdown(\n",
    "    text: str,\n",
    "    max_tokens: int = 200,\n",
    "    overlap: int = 50\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Splits Markdown text into chunks at block-level elements\n",
    "    (headings, paragraphs, lists, tables, code, blockquotes).\n",
    "    Adds overlap (in words) between all consecutive chunks.\n",
    "    \n",
    "    Args:\n",
    "        text: The markdown text to be chunked\n",
    "        max_tokens: Maximum number of words per chunk\n",
    "        overlap: Number of overlapping words between consecutive chunks\n",
    "    \n",
    "    Returns:\n",
    "        List of text chunks with specified overlap\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize markdown parser to understand document structure\n",
    "    md = MarkdownIt()\n",
    "    tokens = md.parse(text)\n",
    "\n",
    "    # Group tokens into block-level segments to preserve markdown structure\n",
    "    # This ensures we don't split in the middle of headings, lists, etc.\n",
    "    blocks = []\n",
    "    buf = []\n",
    "    for tok in tokens:\n",
    "        if tok.block and tok.type.endswith(\"_open\"):\n",
    "            buf = []\n",
    "        elif tok.block and tok.type.endswith(\"_close\"):\n",
    "            if buf:\n",
    "                blocks.append(\"\\n\".join(buf).strip())\n",
    "                buf = []\n",
    "        elif tok.content:\n",
    "            buf.append(tok.content)\n",
    "    if buf:\n",
    "        blocks.append(\"\\n\".join(buf).strip())\n",
    "\n",
    "    # Split blocks into chunks with overlap to maintain context continuity\n",
    "    chunks = []\n",
    "    current_words = []\n",
    "    for block in blocks:\n",
    "        words = block.split()\n",
    "        for w in words:\n",
    "            current_words.append(w)\n",
    "            if len(current_words) >= max_tokens:\n",
    "                # Emit a complete chunk\n",
    "                chunks.append(\" \".join(current_words))\n",
    "                # Prepare next buffer with overlap from the end of this chunk\n",
    "                # This ensures context continuity between chunks\n",
    "                current_words = current_words[-overlap:] if overlap > 0 else []\n",
    "\n",
    "    # Add any remaining words as the final chunk\n",
    "    if current_words:\n",
    "        chunks.append(\" \".join(current_words))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def save_chunks_to_jsonl(chunks, filename):\n",
    "    \"\"\"\n",
    "    Save a list of strings to a JSONL file where each line is a JSON object\n",
    "    with the key 'chunk'. Returns the Path to the saved file.\n",
    "\n",
    "    Args:\n",
    "        chunks (list of str): List of text chunks to save.\n",
    "        filename (str): Path to the output .jsonl file (string or Path).\n",
    "\n",
    "    Returns:\n",
    "        pathlib.Path: Path to the saved file.\n",
    "    \"\"\"\n",
    "    path = Path(filename)\n",
    "    with path.open('w', encoding='utf-8') as f:\n",
    "        for chunk in chunks:\n",
    "            json_line = json.dumps({\"chunk\": chunk}, ensure_ascii=False)\n",
    "            f.write(json_line + '\\n')\n",
    "    print(f\"Saved {len(chunks)} chunks to {path}\")\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f67177b-d35c-419d-880f-d066d91f2afc",
   "metadata": {},
   "source": [
    "## Chunk Markdown\n",
    "\n",
    "Markdown files will be broken down into chunks at least `max_tokens` in length.\n",
    "\n",
    "Utilize the utility function `chunk_markdown` to chunk the markdown file into smaller pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97789ea-434d-41cd-832b-aad957ab1c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = chunk_markdown(text, max_tokens=5000, overlap=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b2d9e7-0ccb-45d9-9cd8-bc2457a5c9ab",
   "metadata": {},
   "source": [
    "## (Optional) Save Chunks to intermediate chunks.jsonl\n",
    "\n",
    "The intermediate `chunks.jsonl` file can be used to tweak chunks before proceeding to seed dataset creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b856acc-aecf-4a83-83af-5dccc326bb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_path = save_chunks_to_jsonl(chunks, f\"{OUTPUT_DIR}/chunks.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656c6af0-d094-448d-85dc-4599a0937374",
   "metadata": {},
   "source": [
    "## (Optional) Review size of Chunks\n",
    "\n",
    "Chunks should be between 6-8K tokens in length. Chunks that are not within this range (excluding the final chunk) should be merged or split apart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6153287d-19ed-42a9-9d1f-bf793fdfdad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "i = 1\n",
    "min_tokens = 6000\n",
    "max_tokens = 8000\n",
    "for chunk in chunks:\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    token_count = len(enc.encode(chunk))\n",
    "    if (token_count < min_tokens or token_count > max_tokens) and (i != len(chunks)):\n",
    "        print(f\"\\033[31mWARNING: Chunk {i} ({chunk[:30]} ... {chunk[-30:]}) {token_count} tokens\\033[0m\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae45260e-b816-430f-98c1-c0d98adb3f30",
   "metadata": {},
   "source": [
    "## Load Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddc7c2b-6957-44c1-811b-2e950a540e74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "chunks_files = [f\"{OUTPUT_DIR}/chunks.jsonl\"]\n",
    "\n",
    "# Load the dataset from the JSON file\n",
    "chunks = load_dataset(\"json\", data_files=chunks_files).rename_columns({'chunk': 'document'}).select_columns('document')\n",
    "# chunks is a DatasetDict. By default the Dataset for the chunks is getting put in the \"train\" split in the DatasetDict\n",
    "chunks = chunks['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a445a5-1497-4058-8431-c81e7ee9e06d",
   "metadata": {},
   "source": [
    "## Set ICL Fields\n",
    "\n",
    "The seed data requires the following fields:\n",
    "   - `document_outline`: A concise title or summary that accurately represents the entire document.\n",
    "     For documents covering multiple themes, consider providing multiple outlines (one per section).\n",
    "   - `domain`: The domain or subject area of the document.\n",
    "   - `icl_document`: A ~500 token representative sample extracted from the document. This may include paragraphs, bulleted lists, tables, code snippets, definitions, etc.\n",
    "   - `icl_query_1`, `icl_query_2`, `icl_query_3`: Three questions based on the `icl_document` sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7614dc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_outline = \"International Business Machines (IBM) annual company earnings report 2024\"\n",
    "\n",
    "domain = \"Finance\"\n",
    "\n",
    "icl_document = \"\"\"In 2024, we reported $62.8 billion in revenue, income from continuing operations of $6.0 billion, \n",
    "which includes the impact of the pension settlement charges of $3.1 billion ($2.4 billion net of tax), \n",
    "and operating (non-GAAP) earnings of $9.7 billion, which excludes the impact of the pension settlement charges. \n",
    "Refer to \"Organization of Information,\" for additional information. \n",
    "Diluted earnings per share from continuing operations was $6.42 as reported, \n",
    "including an impact of $2.57 from the pension settlement charges, and diluted earnings per share was $10.33 on an operating (non-GAAP) basis. \n",
    "We generated $13.4 billion in cash from operations and $12.7 billion in free cash flow, and returned $6.1 billion to shareholders in dividends. \n",
    "We are pleased with the progress we made in 2024, delivering revenue growth in our re-positioned business and strong cash flow generation. \n",
    "Our 2024 performance demonstrates the success of our focused strategy, enhanced portfolio and sustainable revenue growth. \n",
    "We increased our investment in innovation and talent and completed eleven acquisitions in 2024, \n",
    "strengthening our hybrid cloud and AI capabilities, all while continuing to return value to shareholders through our dividend.\n",
    "\n",
    "Total revenue grew 1.4 percent year to year as reported and 3 percent adjusted for currency compared to the prior year, \n",
    "led by our Software performance. Software revenue increased 8.3 percent as reported and 9.0 percent adjusted for currency, \n",
    "with strength across our portfolio. Hybrid Platform & Solutions increased 8.1 percent as reported and 8.7 percent adjusted for currency, \n",
    "reflecting growth across all lines of business with double-digit revenue growth in Red Hat and Automation. \n",
    "Transaction Processing increased 8.7 percent as reported and 9.6 percent adjusted for currency, with growth in both recurring and transactional revenue. \n",
    "Consulting revenue decreased 0.9 percent as reported but grew 0.6 percent adjusted for currency, \n",
    "and continued to be impacted by a dynamic market environment as clients reprioritized spending. \n",
    "Infrastructure decreased 3.9 percent year to year as reported and 2.7 percent adjusted for currency, reflecting product cycle dynamics.\n",
    "\"\"\"\n",
    "\n",
    "icl_query_1 = \"What was the 2024 revenue in billions of dollars?\"\n",
    "icl_query_2 = \"How much did infrastruture decrease year to year?\"\n",
    "icl_query_3 = \"What did the IBM 2024 performance demonstrate?\"\n",
    "\n",
    "\n",
    "icl = {\n",
    "    \"document_outline\": document_outline,\n",
    "    \"icl_document\": icl_document,\n",
    "    \"icl_query_1\": icl_query_1,\n",
    "    \"icl_query_2\": icl_query_2,\n",
    "    \"icl_query_3\": icl_query_3,\n",
    "    \"domain\": domain,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b335ab8b-13b4-4279-8f20-b2830f11d5c2",
   "metadata": {},
   "source": [
    "## Map ICL Fields to Document Chunks and Write `seed_data.jsonl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b2f6fb-bc92-48d2-9794-1f14ca6804ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the ICL fields to each document chunk (if you want to use the same ICL for all, as shown here)\n",
    "seed_data = chunks.map(lambda x: icl)\n",
    "\n",
    "# Save the seed data to a JSONL file for downstream use\n",
    "seed_data.to_json(f'{OUTPUT_DIR}/seed_data.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f3ff7f",
   "metadata": {},
   "source": [
    "### Next Steps:\n",
    "- The seed_data.jsonl file is now ready for the knowledge tuning pipeline.\n",
    "- You can now refer to the [knowledge generation](../02_Knowledge_Generation/Knowledge_Generation.ipynb) notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6af0f6e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-utils",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
