{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4535f5ad",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "This notebook covers the required preprocessing steps for preparing the `seed.jsonl` dataset which is ready for Synthetic Data Generation (SDG). \n",
    "\n",
    "1. Configure the paths\n",
    "2. Loading the files into memory\n",
    "3. Document Conversion\n",
    "4. Chunking\n",
    "5. Saving the processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e7981d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05b27fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKSPACE = Path.cwd().parent # Path to the workspace directory\n",
    "\n",
    "SOURCE_DOCUMENT_DIR= WORKSPACE / \"source_documents\"\n",
    "OUTPUT_DIR= WORKSPACE / \"output\" / \"step_01\"\n",
    "\n",
    "\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)  # Create output directory if it doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e411e44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total PDF files found: 2 \n",
      "\n",
      "Available Files:\n",
      "\t2022-nfl-rulebook.pdf\n",
      "\t2023-nfl-rulebook.pdf\n"
     ]
    }
   ],
   "source": [
    "available_files = SOURCE_DOCUMENT_DIR.glob(\"**/*.pdf\")\n",
    "available_files = list(available_files)\n",
    "\n",
    "print(f\"Total PDF files found: {len(available_files)} \\n\")\n",
    "print(\"Available Files:\")\n",
    "\n",
    "for file in available_files:\n",
    "    print(f\"\\t{file.stem}.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975c02fd",
   "metadata": {},
   "source": [
    "## Document Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82694f1f",
   "metadata": {},
   "source": [
    "The source documents are in pdf format and we will be using `docling` to read and convert them into docling output format.\n",
    "\n",
    "configuring docling pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "31ee7c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.datamodel.accelerator_options import AcceleratorOptions, AcceleratorDevice\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import EasyOcrOptions, PdfPipelineOptions, VlmPipelineOptions, smoldocling_vlm_conversion_options\n",
    "\n",
    "from docling.pipeline.vlm_pipeline import VlmPipeline\n",
    "from docling.backend.docling_parse_v4_backend import DoclingParseV4DocumentBackend\n",
    "\n",
    "pipeline_options = PdfPipelineOptions()\n",
    "doc_converter = DocumentConverter(\n",
    "    format_options={\n",
    "        InputFormat.PDF: PdfFormatOption(\n",
    "            pipeline_options=pipeline_options\n",
    "        )\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a33fb2",
   "metadata": {},
   "source": [
    "Convert the document to docling format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "960baf76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-19 11:37:37,452 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-09-19 11:37:37,466 - INFO - Going to convert document batch...\n",
      "2025-09-19 11:37:37,466 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e647edf348883bed75367b22fbe60347\n",
      "2025-09-19 11:37:37,468 - INFO - Accelerator device: 'mps'\n",
      "2025-09-19 11:37:40,656 - INFO - Accelerator device: 'mps'\n",
      "2025-09-19 11:37:42,238 - INFO - Accelerator device: 'mps'\n",
      "2025-09-19 11:37:42,934 - INFO - Processing document 2022-nfl-rulebook.pdf\n",
      "2025-09-19 11:37:47,062 - INFO - Finished converting document 2022-nfl-rulebook.pdf in 9.62 sec.\n",
      "2025-09-19 11:37:47,073 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-09-19 11:37:47,077 - INFO - Going to convert document batch...\n",
      "2025-09-19 11:37:47,078 - INFO - Processing document 2023-nfl-rulebook.pdf\n",
      "2025-09-19 11:37:52,503 - INFO - Finished converting document 2023-nfl-rulebook.pdf in 5.43 sec.\n"
     ]
    }
   ],
   "source": [
    "confidence_report = {}\n",
    "for file in available_files:\n",
    "\n",
    "    conv_result = doc_converter.convert(file)\n",
    "\n",
    "    document = conv_result.document\n",
    "    confidence_report[file.stem] = conv_result.confidence\n",
    "\n",
    "    document_dict = document.export_to_dict()\n",
    "\n",
    "    (OUTPUT_DIR / \"docling_output\").mkdir(parents=True, exist_ok=True)\n",
    "    output_file = OUTPUT_DIR / \"docling_output\" /  f\"{file.stem}.json\"\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(document_dict, f, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "318f18a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion confidence for 2022-nfl-rulebook:\n",
      "Average confidence: \u001b[1mEXCELLENT\u001b[0m (score 0.917)\n",
      "Pages that scored lower than average: 1, 2\n",
      "\n",
      "Conversion confidence for 2023-nfl-rulebook:\n",
      "Average confidence: \u001b[1mEXCELLENT\u001b[0m (score 0.938)\n",
      "Pages that scored lower than average: 1, 2, 6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for file, confidence_report in confidence_report.items():\n",
    "    print(f\"Conversion confidence for {file}:\")\n",
    "    \n",
    "    print(f\"Average confidence: \\x1b[1m{confidence_report.mean_grade.name}\\033[0m (score {confidence_report.mean_score:.3f})\")\n",
    "    \n",
    "    low_score_pages = []\n",
    "    for page in confidence_report.pages:\n",
    "        page_confidence_report = confidence_report.pages[page]\n",
    "        if page_confidence_report.mean_score < confidence_report.mean_score:\n",
    "            low_score_pages.append(page)\n",
    "\n",
    "    print(f\"Pages that scored lower than average: {', '.join(str(x + 1) for x in low_score_pages)}\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3178dc",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008c0ac9",
   "metadata": {},
   "source": [
    "Chunk the document using docling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6a0b01b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.chunking import HybridChunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4e1404e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker = HybridChunker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ec1add96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-19 11:37:52,914 - INFO - detected formats: [<InputFormat.JSON_DOCLING: 'json_docling'>]\n",
      "2025-09-19 11:37:52,921 - INFO - Going to convert document batch...\n",
      "2025-09-19 11:37:52,922 - INFO - Initializing pipeline for SimplePipeline with options hash 995a146ad601044538e6a923bea22f4e\n",
      "2025-09-19 11:37:52,922 - INFO - Processing document 2023-nfl-rulebook.json\n",
      "2025-09-19 11:37:52,923 - INFO - Finished converting document 2023-nfl-rulebook.json in 0.01 sec.\n",
      "2025-09-19 11:37:53,004 - INFO - detected formats: [<InputFormat.JSON_DOCLING: 'json_docling'>]\n",
      "2025-09-19 11:37:53,009 - INFO - Going to convert document batch...\n",
      "2025-09-19 11:37:53,010 - INFO - Processing document 2022-nfl-rulebook.json\n",
      "2025-09-19 11:37:53,010 - INFO - Finished converting document 2022-nfl-rulebook.json in 0.01 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created for 2023-nfl-rulebook: 25\n",
      "Total chunks created for 2022-nfl-rulebook: 22\n",
      "Path of chunks JSON is: /Users/scharan/projects/red-hat-ai-examples/examples/knowledge-tuning/output/step_01/chunks.jsonl\n"
     ]
    }
   ],
   "source": [
    "all_chunks = []\n",
    "\n",
    "json_files = (OUTPUT_DIR / \"docling_output\").glob(\"*.json\")\n",
    "\n",
    "convertor = DocumentConverter()\n",
    "\n",
    "for file in json_files:\n",
    "    conv_result = convertor.convert(file)\n",
    "\n",
    "    chunks = chunker.chunk(conv_result.document)\n",
    "    chunks = list(chunks)\n",
    "    print(f\"Total chunks created for {file.stem}: {len(chunks)}\")\n",
    "\n",
    "    for chunk in chunks:\n",
    "        all_chunks.append({\n",
    "            \"chunk\":chunker.contextualize(chunk),\n",
    "            \"file\": file.stem,\n",
    "            \"metadata\":chunk.meta.export_json_dict()\n",
    "        })\n",
    "\n",
    "chunks_file_path = OUTPUT_DIR / \"chunks.jsonl\"\n",
    "with open(chunks_file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    for chunk in all_chunks:\n",
    "        json.dump(chunk, file)\n",
    "        file.write(\"\\n\")\n",
    "    print(f\"Path of chunks JSON is: {Path(chunks_file_path).resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295bfb8d",
   "metadata": {},
   "source": [
    "View random chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "19c4510c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Randomly selected chunk 1: ==========\n",
      "\n",
      "A.R. 15.262        Double Challenge\n",
      "Second-and-10 on B35. With 12:00 remaining in the fourth quarter, QBA1 rolls out and throws a pass to A2 at the back of the end zone that is ruled incomplete. Team A challenges that the pass was complete, but replays show that A2 only got one foot down inbounds and the call on the field is upheld. While the Referee is making his announcement, a new replay comes up that shows the QB stepping on the sideline at the B40 before releasing the pass. Team B challenges the play.\n",
      "Ruling: Reviewable. Both teams can challenge the same play. A's ball third-and-15 on the B40, reset the clock to the time when the QB stepped out of bounds, and start on the snap. A team cannot challenge the same play twice. It is important that all reviewable aspects of a play are confirmed by replay regardless of what is being challenged. Team A is charged with a challenge and a timeout.\n",
      "\n",
      "\n",
      "== Randomly selected chunk 2: ==========\n",
      "\n",
      "A.R. 15.262 Double Challenge\n",
      "Second-and-10 on B35. With 12:00 remaining in the fourth quarter, QBA1 rolls out and throws a pass to A2 at the back of the end zone that is ruled incomplete. Team A challenges that the pass was complete, but replays show that A2 only got one foot down inbounds and the call on the field is upheld. While the Referee is making his announcement, a new replay comes up that shows the QB stepping on the sideline at the B40 before releasing the pass. Team B challenges the play.\n",
      "Ruling: Reviewable. Both teams can challenge the same play. A's ball third-and-15 on the B40, reset the clock to the time when the QB stepped out of bounds, and start on the snap. A team cannot challenge the same play twice. It is important that all reviewable aspects of a play are confirmed by replay regardless of what is being challenged. Team A is charged with a challenge and a timeout.\n",
      "\n",
      "\n",
      "== Randomly selected chunk 3: ==========\n",
      "\n",
      "A.R. 15.257 Play not ruled score\n",
      "Third-and-goal on B4. In the third quarter, back A2 takes a handoff and runs to the goal line where he is hit and driven backward. The officials spot the ball short of the goal line and make it fourth down. Replays show the ball broke the plane of the goal line. Ruling: Reviewable. Touchdown, KO A35, reset the clock to the time of the touchdown. Since the ruling on the field did not result in points for either team, the Team A coach must challenge the ruling outside two minutes of either half.\n",
      "\n",
      "\n",
      "== Randomly selected chunk 4: ==========\n",
      "\n",
      "A.R. 15.264        Try\n",
      "First-and-10 on B15. At the start of the play there is 2:02 remaining in the second quarter. A2 catches a pass in the end zone that is ruled a TD with 1:53 left on the clock. Team A then attempts a two-point conversion and QBA1 is ruled down short of the goal line. Replays show that the ball broke the plane before he was down.\n",
      "Ruling: Reviewable, two-point conversion is good, KO A35. Only the Replay Official can initiate a review of a Try, whether successful or unsuccessful.\n",
      "\n",
      "\n",
      "== Randomly selected chunk 5: ==========\n",
      "\n",
      "A.R. 15.256 Multiple aspects to review\n",
      "First-and-10 on A30. With 10:00 remaining in the second quarter, back A2 fumbles at the A45 and it is returned by B4 to the A5. Replays show that after recovering the fumble, B4 was down by contact at the A45.\n",
      "Ruling: Reviewable. B's ball first-and-10 on A45, reset the clock to the time when B4 was down by contact. The ruling on the field was a fumble recovered by the defense. Although that is not in question, only the Replay Official can initiate a review of this play. Once the review is initiated, replay will look at all reviewable aspects of the play.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NUM_CHUNKS_TO_VIEW = 5\n",
    "\n",
    "import random\n",
    "import json\n",
    "\n",
    "sample = random.sample(all_chunks, min(len(all_chunks), NUM_CHUNKS_TO_VIEW))\n",
    "\n",
    "i = 1\n",
    "for chunk in sample:\n",
    "    print(f\"== Randomly selected chunk {i}: ==========\\n\\n{chunk['chunk']}\\n\\n\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3179f5f6",
   "metadata": {},
   "source": [
    "Read the chunks back from the saved file and then randomly select the chunks to use as seed data for SDG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "52e275e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path of selected chunks JSON is: /Users/scharan/projects/red-hat-ai-examples/examples/knowledge-tuning/output/step_01/selected_chunks.jsonl\n"
     ]
    }
   ],
   "source": [
    "chunks = []\n",
    "with open(chunks_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        chunk = json.loads(line)\n",
    "        chunks.append(chunk)\n",
    "\n",
    "\n",
    "\n",
    "NUM_SEED_EXAMPLES = 5 # Number of chunks to select as seed examples\n",
    "selected_chunks =random.sample(chunks,NUM_SEED_EXAMPLES)\n",
    "\n",
    "selected_chunks_path = OUTPUT_DIR / \"selected_chunks.jsonl\"\n",
    "with open(selected_chunks_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    for chunk in selected_chunks:\n",
    "        json.dump(chunk, file)\n",
    "        file.write(\"\\n\")\n",
    "    print(f\"Path of selected chunks JSON is: {Path(selected_chunks_path).resolve()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee6682e",
   "metadata": {},
   "source": [
    "Generate QnA for each chunk selected above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b1899f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your API credentials and model details\n",
    "API_KEY =\"d7bbaa22df39cb51374efa3b0a64962d\"\n",
    "ENDPOINT = \"https://granite-3-3-8b-instruct-maas-apicast-production.apps.prod.rhoai.rh-aiservices-bu.com:443/v1\"\n",
    "MODEL_NAME = \"granite-3-3-8b-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "55f617e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOMISATION_PROMPT = \"Generate atleast 5 seed examples in the format specified below.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6b0ffc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai_tools.usecase.knowledge_tuning.utils import generate_seed_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b7b5fdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai_tools.usecase.knowledge_tuning.utils import view_seed_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "488bf167",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/scharan/projects/red-hat-ai-examples/examples/knowledge-tuning/output/step_01/qna.yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mview_seed_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOUTPUT_DIR\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mqna.yaml\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/red-hat-ai-examples/src/ai_tools/usecase/knowledge_tuning/utils.py:268\u001b[39m, in \u001b[36mview_seed_example\u001b[39m\u001b[34m(qna_output_path, seed_example_num)\u001b[39m\n\u001b[32m    258\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mview_seed_example\u001b[39m(qna_output_path: Path, seed_example_num: \u001b[38;5;28mint\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    259\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[33;03m    View a specific seed example in a qna.yaml\u001b[39;00m\n\u001b[32m    261\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    265\u001b[39m \u001b[33;03m        None\u001b[39;00m\n\u001b[32m    266\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m268\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mqna_output_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m yaml_file:\n\u001b[32m    269\u001b[39m         yaml_data = yaml.safe_load(yaml_file)\n\u001b[32m    270\u001b[39m         seed_examples = yaml_data.get(\u001b[33m'\u001b[39m\u001b[33mseed_examples\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/Users/scharan/projects/red-hat-ai-examples/examples/knowledge-tuning/output/step_01/qna.yaml'"
     ]
    }
   ],
   "source": [
    "view_seed_example(OUTPUT_DIR / \"qna.yaml\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6788f2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviewing seed examples file at /Users/scharan/projects/red-hat-ai-examples/examples/knowledge-tuning/output/step_01/qna.yaml\n",
      "Found contribution summary...\n",
      "Found 'domain'...\n",
      "Found 4 'contexts' in 'sed_examples'. Minimum expected number is 4...\n",
      "Seed Example 1 contains expected number (3) of 'question_and_answers'...\n",
      "Seed Example 2 contains expected number (3) of 'question_and_answers'...\n",
      "Seed Example 3 contains expected number (3) of 'question_and_answers'...\n",
      "Seed Example 4 contains expected number (3) of 'question_and_answers'...\n",
      "Seed Examples YAML /Users/scharan/projects/red-hat-ai-examples/examples/knowledge-tuning/output/step_01/qna.yaml is valid :)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ai_tools.usecase.knowledge_tuning.utils import review_seed_examples_file\n",
    "\n",
    "\n",
    "review_seed_examples_file(OUTPUT_DIR / \"qna.yaml\",min_seed_examples=4,num_qa_pairs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c493bc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -qq datasets transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab1405d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai_tools.usecase.knowledge_tuning.create_seed_dataset import get_seed_dataset, safe_concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b6b84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "Map: 100%|██████████| 25/25 [00:00<00:00, 3196.49 examples/s]\n",
      "Map: 100%|██████████| 25/25 [00:00<00:00, 7271.68 examples/s]\n",
      "Map: 100%|██████████| 25/25 [00:00<00:00, 7929.34 examples/s]\n",
      "Map: 100%|██████████| 25/25 [00:00<00:00, 7612.72 examples/s]\n",
      "Filter: 100%|██████████| 100/100 [00:00<00:00, 3323.91 examples/s]\n",
      "Map: 100%|██████████| 22/22 [00:00<00:00, 5937.88 examples/s]\n",
      "Map: 100%|██████████| 22/22 [00:00<00:00, 7012.82 examples/s]\n",
      "Map: 100%|██████████| 22/22 [00:00<00:00, 6626.07 examples/s]\n",
      "Map: 100%|██████████| 22/22 [00:00<00:00, 7082.80 examples/s]\n",
      "Filter: 100%|██████████| 88/88 [00:00<00:00, 3646.75 examples/s]\n"
     ]
    }
   ],
   "source": [
    "seed_data = get_seed_dataset(OUTPUT_DIR,OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a8e9a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['document', 'document_outline', 'document_title', 'domain', 'icl_document', 'icl_query_1', 'icl_response_1', 'icl_query_2', 'icl_response_2', 'icl_query_3', 'icl_response_3'],\n",
       "    num_rows: 172\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04f1a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 189.00ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "393338"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_data.to_json(OUTPUT_DIR/ \"final_seed_data.jsonl\",orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e9d21e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "01_Data_Preprocessing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
